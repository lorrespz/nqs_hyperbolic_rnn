{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf66e9db-dc84-437a-b8af-b042815ae098",
   "metadata": {},
   "source": [
    "# 1D J1=1.0, J2=0.8: Training with gradient clipping\n",
    "\n",
    "This notebook is part of the work arXiv:2505.22083 (https://arxiv.org/abs/2505.22083), \"Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz\". Code written by HLD. \n",
    "\n",
    "In this notebook, we collected the best training results from training with gradient clipping by value and by global norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9abff75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:57:05.552643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with gradient clipping by global norm of 9.0 (both Euclidean and hyperbolic)\n",
      "The norm clipping is defined in the Adam optimizer\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "#Training with gradient clipping\n",
    "import sys\n",
    "sys.path.append('../../utility')\n",
    "from j1j2_hyprnn_train_loop_grad_clipping_gn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fd866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_exact = -20.9841\n",
    "syssize = 50\n",
    "nssamples = 50\n",
    "J1 = 1.0\n",
    "J2 = 0.8\n",
    "var_tol = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2f89e",
   "metadata": {},
   "source": [
    "# EuclGRU\n",
    "\n",
    "- The energy curve with no gradient clipping showed some kinks and was overall not smooth.\n",
    "- GC by value in the range [-1,1] led to a worse result with a converged energy value being far away from the correct value.\n",
    "- GC by global norm of 8.0 made the curve smoother but also at a very slightly higher converged energy compared to the no-GC case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd289b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<j1j2_hyprnn_wf.rnn_eucl_wf at 0x7fcf204b7b50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type = 'EuclGRU'\n",
    "hidden_units = 75\n",
    "wf_egru = rnn_eucl_wf(syssize, cell_type, hidden_units)\n",
    "wf_egru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8a74d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -1.37457, mean energy: 21.62602+0.10530j, varE: 1.11107\n",
      "step: 10, loss: 19.63758, mean energy: -3.92291-0.02272j, varE: 12.71488\n",
      "step: 20, loss: -0.26210, mean energy: -8.55120+0.30371j, varE: 7.94827\n",
      "step: 30, loss: -7.23647, mean energy: -9.89894+0.03235j, varE: 14.51643\n",
      "step: 40, loss: 4.14861, mean energy: -9.67870+0.18526j, varE: 2.90372\n",
      "step: 50, loss: -6.01268, mean energy: -11.30545+0.16722j, varE: 2.46759\n",
      "step: 60, loss: -4.62308, mean energy: -11.19780-0.08426j, varE: 5.25986\n",
      "step: 70, loss: -0.80757, mean energy: -11.82998+0.18075j, varE: 13.72872\n",
      "step: 80, loss: 0.97972, mean energy: -11.13771+0.13303j, varE: 4.90203\n",
      "step: 90, loss: 10.39035, mean energy: -11.97471-0.36018j, varE: 8.09400\n",
      "step: 100, loss: 2.78839, mean energy: -11.36987-0.55350j, varE: 6.05023\n",
      "step: 110, loss: -2.45224, mean energy: -14.45003-0.01314j, varE: 6.97956\n",
      "step: 120, loss: 4.46085, mean energy: -14.54065+0.32027j, varE: 5.57924\n",
      "step: 130, loss: 1.66693, mean energy: -14.08788-0.12581j, varE: 10.06054\n",
      "step: 140, loss: -3.74286, mean energy: -16.47036+0.07444j, varE: 6.54609\n",
      "step: 150, loss: -1.59448, mean energy: -16.98751+0.05433j, varE: 4.71593\n",
      "step: 160, loss: -1.72531, mean energy: -17.17225-0.11422j, varE: 8.23702\n",
      "Best model saved at epoch 167 with best E=-17.78608+0.00288j, varE=1.80005\n",
      "step: 170, loss: 0.11414, mean energy: -16.89185+0.17775j, varE: 3.97687\n",
      "Best model saved at epoch 179 with best E=-17.99338+0.07969j, varE=1.63300\n",
      "step: 180, loss: 1.37106, mean energy: -17.99157+0.01871j, varE: 1.57582\n",
      "Best model saved at epoch 186 with best E=-18.10206+0.15904j, varE=1.32243\n",
      "step: 190, loss: -3.59872, mean energy: -17.65388+0.06915j, varE: 4.81260\n",
      "step: 200, loss: 0.35202, mean energy: -17.88433+0.00494j, varE: 1.27044\n",
      "step: 210, loss: -4.68506, mean energy: -17.50010-0.09473j, varE: 2.40200\n",
      "Best model saved at epoch 213 with best E=-18.10657+0.07312j, varE=1.61055\n",
      "step: 220, loss: -0.14310, mean energy: -18.03615-0.09376j, varE: 1.16077\n",
      "Best model saved at epoch 229 with best E=-18.12548+0.06443j, varE=1.05666\n",
      "step: 230, loss: 8.25330, mean energy: -18.06243-0.36207j, varE: 2.04383\n",
      "Best model saved at epoch 235 with best E=-18.22190+0.00355j, varE=1.45543\n",
      "Best model saved at epoch 239 with best E=-18.33541+0.08185j, varE=1.09732\n",
      "step: 240, loss: -4.72720, mean energy: -17.87441-0.08682j, varE: 1.79159\n",
      "Best model saved at epoch 248 with best E=-18.45712-0.00861j, varE=0.87464\n",
      "step: 250, loss: 0.86044, mean energy: -18.08892-0.07027j, varE: 1.58973\n",
      "Best model saved at epoch 256 with best E=-18.48506+0.03794j, varE=1.28135\n",
      "Best model saved at epoch 257 with best E=-18.62191+0.16965j, varE=1.14054\n",
      "step: 260, loss: 1.09726, mean energy: -18.24356+0.07394j, varE: 1.92573\n",
      "Best model saved at epoch 266 with best E=-18.69264-0.02147j, varE=1.03341\n",
      "step: 270, loss: -6.94962, mean energy: -18.56078+0.04659j, varE: 2.27269\n",
      "Best model saved at epoch 271 with best E=-18.80513-0.12642j, varE=1.99035\n",
      "Best model saved at epoch 278 with best E=-18.88777-0.08543j, varE=1.28901\n",
      "step: 280, loss: 3.83011, mean energy: -18.27832-0.18112j, varE: 1.21722\n",
      "Best model saved at epoch 289 with best E=-19.21379+0.03707j, varE=0.65151\n",
      "step: 290, loss: 1.44986, mean energy: -18.93920+0.16790j, varE: 1.37658\n",
      "step: 300, loss: -4.84467, mean energy: -19.12066+0.05108j, varE: 0.71388\n",
      "Best model saved at epoch 303 with best E=-19.29764-0.07062j, varE=0.36911\n",
      "step: 310, loss: 1.44299, mean energy: -18.79455-0.02629j, varE: 0.64536\n",
      "step: 320, loss: -3.26559, mean energy: -18.61893+0.01949j, varE: 2.06025\n",
      "step: 330, loss: 0.07974, mean energy: -19.15106+0.02472j, varE: 0.36978\n",
      "Best model saved at epoch 336 with best E=-19.30147-0.06885j, varE=0.86232\n",
      "step: 340, loss: 0.65353, mean energy: -19.14326-0.09188j, varE: 0.36023\n",
      "Best model saved at epoch 346 with best E=-19.37482+0.07781j, varE=0.78027\n",
      "step: 350, loss: -1.22067, mean energy: -18.95992-0.05523j, varE: 1.63944\n",
      "step: 360, loss: 1.30771, mean energy: -18.96015+0.02207j, varE: 0.94043\n",
      "step: 370, loss: -1.12704, mean energy: -18.90250+0.00084j, varE: 1.35244\n",
      "Best model saved at epoch 379 with best E=-19.38566+0.06059j, varE=0.14839\n",
      "step: 380, loss: -0.78702, mean energy: -19.18165+0.00661j, varE: 0.43945\n",
      "step: 390, loss: 3.04849, mean energy: -19.14190+0.02467j, varE: 0.86788\n",
      "step: 400, loss: 1.97253, mean energy: -18.99653+0.02003j, varE: 0.87433\n",
      "Best model saved at epoch 405 with best E=-19.40474-0.04022j, varE=0.87626\n",
      "step: 410, loss: 2.42722, mean energy: -19.18834-0.00245j, varE: 0.36811\n",
      "step: 420, loss: 0.19202, mean energy: -19.15922+0.05235j, varE: 0.66783\n",
      "step: 430, loss: -0.67859, mean energy: -19.29692-0.02271j, varE: 0.31274\n",
      "step: 440, loss: -0.95947, mean energy: -19.45760-0.16372j, varE: 3.54884\n",
      "step: 450, loss: 0.92038, mean energy: -19.61554-0.12902j, varE: 2.02564\n",
      "Total time taken: 4.677\n"
     ]
    }
   ],
   "source": [
    "#TRAINING WITH GRADIENT CLIPPING BY GLOBAL NORM 8.0\n",
    "nsteps = 451\n",
    "start = time.time()\n",
    "\n",
    "mE, vE = run_J1J2(wf=wf_egru, numsteps=nsteps, systemsize=syssize, var_tol= var_tol, J1_  = J1, \n",
    "                   J2_ = J2, Marshall_sign = True, \n",
    "                  numsamples = nssamples, learningrate = 1e-2, seed = 111, fname = '../gn_results_grad_clipping_global_norm')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587abf4",
   "metadata": {},
   "source": [
    "# HypGRU\n",
    "- The energy curve with no gradient clipping showed some kinks in the late stage of the training and was overall not smooth.\n",
    "- GC by value in the range [-1,1] led to a worse result with a jump nearing the end of the training\n",
    "- GC by global norm of 9.0 caused the training to be unstable and go towards to opposite value \n",
    "- GC by global norm of 8.0 improved the curve by eliminating the 2 late kinks but it also introduced an earlier kink (way before the convergence was reached). The overall converged value also happened at a slighlty higher value compared to the no-GC case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be59df4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<j1j2_hyprnn_wf.rnn_hyp_wf at 0x7fbb064b7d60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type = 'HypGRU'\n",
    "hidden_units = 75\n",
    "wf_hgru = rnn_hyp_wf(syssize, cell_type, 'hyp', 'id', hidden_units)\n",
    "wf_hgru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4ac16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -2.68951, mean energy: 18.92231+0.30570j, varE: 8.41262\n",
      "step: 10, loss: 12.79593, mean energy: -2.87399+0.01304j, varE: 17.69459\n",
      "step: 20, loss: -12.23351, mean energy: -6.03501-0.93176j, varE: 20.92467\n",
      "step: 30, loss: -10.16785, mean energy: -10.47509+0.20206j, varE: 10.04062\n",
      "step: 40, loss: -17.00056, mean energy: -8.32347-0.17412j, varE: 12.44150\n",
      "step: 50, loss: -12.61209, mean energy: -10.68806-0.41055j, varE: 9.36091\n",
      "step: 60, loss: -3.36862, mean energy: -11.26937+0.18819j, varE: 7.40503\n",
      "step: 70, loss: 3.67706, mean energy: -11.68194+0.45844j, varE: 8.33232\n",
      "step: 80, loss: -7.35777, mean energy: -10.16223-0.19470j, varE: 9.66072\n",
      "step: 90, loss: 4.06979, mean energy: -10.59185+0.08429j, varE: 10.90531\n",
      "step: 100, loss: -9.17120, mean energy: -6.70228-0.46023j, varE: 9.04622\n",
      "step: 110, loss: -3.20023, mean energy: -5.12317-0.04790j, varE: 15.04341\n",
      "step: 120, loss: 4.78419, mean energy: -7.56306-0.07825j, varE: 16.01200\n",
      "step: 130, loss: -12.68428, mean energy: -11.41648+0.16152j, varE: 26.36274\n",
      "step: 140, loss: -7.47665, mean energy: -15.33812-0.44935j, varE: 14.86240\n",
      "step: 150, loss: -2.34921, mean energy: -16.71538+0.26875j, varE: 4.33011\n",
      "step: 160, loss: 0.37521, mean energy: -17.05826-0.11069j, varE: 3.91923\n",
      "step: 170, loss: 8.90286, mean energy: -17.07889+0.14821j, varE: 6.03464\n",
      "step: 180, loss: 3.16791, mean energy: -17.03676+0.07982j, varE: 4.85192\n",
      "Best model saved at epoch 189 with best E=-17.96956+0.04111j, varE=1.85101\n",
      "step: 190, loss: 6.69165, mean energy: -17.34333+0.60346j, varE: 19.10159\n",
      "step: 200, loss: 10.00674, mean energy: -17.15547+0.08340j, varE: 5.43091\n",
      "step: 210, loss: -3.02838, mean energy: -17.55810-0.00380j, varE: 5.64058\n",
      "step: 220, loss: -5.46277, mean energy: -18.02278+0.00528j, varE: 4.17497\n",
      "Best model saved at epoch 224 with best E=-18.59680-0.06243j, varE=1.65497\n",
      "step: 230, loss: -7.39325, mean energy: -18.06313+0.09633j, varE: 7.15254\n",
      "Best model saved at epoch 237 with best E=-18.85712+0.16324j, varE=1.37581\n",
      "step: 240, loss: 4.12076, mean energy: -18.54115-0.04049j, varE: 2.58598\n",
      "Best model saved at epoch 244 with best E=-18.86607+0.02010j, varE=1.37344\n",
      "step: 250, loss: -11.17517, mean energy: -18.26700+0.15944j, varE: 7.44851\n",
      "step: 260, loss: 1.69778, mean energy: -18.26786+0.05670j, varE: 3.11612\n",
      "Best model saved at epoch 267 with best E=-19.12202-0.03413j, varE=0.53710\n",
      "step: 270, loss: 4.33459, mean energy: -18.32291+0.31395j, varE: 22.54574\n",
      "step: 280, loss: -5.50113, mean energy: -17.12115-0.13903j, varE: 3.60393\n",
      "step: 290, loss: 13.63947, mean energy: -18.00345-0.36456j, varE: 9.56423\n",
      "step: 300, loss: 4.42145, mean energy: -18.16477+0.05899j, varE: 3.03523\n",
      "step: 310, loss: -3.54010, mean energy: -18.46832-0.10394j, varE: 1.96654\n",
      "Best model saved at epoch 318 with best E=-19.42640+0.06160j, varE=0.33467\n",
      "step: 320, loss: 0.03000, mean energy: -19.13544-0.03943j, varE: 1.31462\n",
      "step: 330, loss: 0.74933, mean energy: -18.43681-0.01489j, varE: 2.53580\n",
      "step: 340, loss: 2.55914, mean energy: -19.48248-0.05363j, varE: 2.94372\n",
      "step: 350, loss: -1.28778, mean energy: -19.25958+0.12626j, varE: 1.49727\n",
      "step: 360, loss: 3.07953, mean energy: -18.91346+0.20372j, varE: 2.10028\n",
      "Best model saved at epoch 366 with best E=-19.53157-0.06254j, varE=0.58800\n",
      "step: 370, loss: -1.39975, mean energy: -19.32447-0.15295j, varE: 1.97782\n",
      "Best model saved at epoch 379 with best E=-19.66133+0.06100j, varE=0.74686\n",
      "step: 380, loss: -0.87000, mean energy: -19.30557+0.04419j, varE: 1.26608\n",
      "step: 390, loss: -2.21884, mean energy: -19.26023-0.00577j, varE: 1.47692\n",
      "step: 400, loss: -3.22498, mean energy: -19.59436-0.01976j, varE: 1.22883\n",
      "step: 410, loss: -1.72205, mean energy: -19.44769+0.16723j, varE: 1.20715\n",
      "Best model saved at epoch 411 with best E=-19.82500+0.02635j, varE=0.50518\n",
      "Best model saved at epoch 415 with best E=-19.85996-0.01853j, varE=0.51099\n",
      "step: 420, loss: 7.31543, mean energy: -19.76970+0.08973j, varE: 2.94313\n",
      "Best model saved at epoch 430 with best E=-19.92192+0.05546j, varE=0.48594\n",
      "step: 430, loss: -0.08630, mean energy: -19.92192+0.05546j, varE: 0.48594\n",
      "step: 440, loss: -0.86978, mean energy: -19.60232-0.20132j, varE: 3.28484\n",
      "step: 450, loss: -4.31061, mean energy: -19.56100-0.01517j, varE: 1.32422\n",
      "Total time taken: 29.854\n"
     ]
    }
   ],
   "source": [
    "#TRAINING WITH GRADIENT CLIPPING BY GLOBAL NORM 8.0\n",
    "nsteps=451\n",
    "start = time.time()\n",
    "mE, vE = run_J1J2_hypvars(wf=wf_hgru, numsteps=nsteps, systemsize=syssize, var_tol=var_tol,\n",
    "                          J1_ = J1, J2_ = J2, Marshall_sign = True, \n",
    "                           numsamples = nssamples,  lr1=1e-2, lr2=1e-2, seed = 111, fname = '../gn_results_grad_clipping_global_norm')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
