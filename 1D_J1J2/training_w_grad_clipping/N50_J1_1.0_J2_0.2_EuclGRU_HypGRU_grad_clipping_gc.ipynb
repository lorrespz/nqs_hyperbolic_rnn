{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb66cdf-c3ad-4163-9a29-cf369464e069",
   "metadata": {},
   "source": [
    "# 1D J1=1.0, J2=0.2: Training with gradient clipping\n",
    "\n",
    "This notebook is part of the work arXiv:2505.22083 (https://arxiv.org/abs/2505.22083), \"Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz\". Code written by HLD. \n",
    "\n",
    "In this notebook, we collected the best training results from training with gradient clipping by value and by global norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9abff75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 19:37:22.329153: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with gradient clipping by global norm of 9.0 (both Euclidean and hyperbolic)\n",
      "The norm clipping is defined in the Adam optimizer\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../utility')\n",
    "from j1j2_hyprnn_train_loop_grad_clipping_gn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fd866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_exact = -20.3150\n",
    "syssize = 50\n",
    "nssamples = 50\n",
    "J1 = 1.0\n",
    "J2 = 0.2\n",
    "nsteps = 401\n",
    "var_tol = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2f89e",
   "metadata": {},
   "source": [
    "# EuclGRU\n",
    "- The energy curve with no gradient clipping showed a big jump which caused the convergence to occur at a high value.\n",
    "- Clipping by value eliminated the jump but still led to a large bump in the curve. \n",
    "-  Clipping by global norm eliminated the jump (with only 2 small kinks before the convergence is reached) and overall improved the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6ddbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<j1j2_hyprnn_wf.rnn_eucl_wf at 0x1aa96aa50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type = 'EuclGRU'\n",
    "hidden_units = 75\n",
    "wf_egru = rnn_eucl_wf(syssize, cell_type, hidden_units)\n",
    "wf_egru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66c9825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -0.73938, mean energy: 14.45013+0.08308j, varE: 0.32071\n",
      "step: 10, loss: 3.19461, mean energy: -3.29343-0.04501j, varE: 9.89220\n",
      "step: 20, loss: -2.86215, mean energy: -7.93687+0.06412j, varE: 6.51807\n",
      "step: 30, loss: -2.64299, mean energy: -13.70494+0.01687j, varE: 9.23479\n",
      "step: 40, loss: -3.08258, mean energy: -13.84703-0.09183j, varE: 5.69278\n",
      "step: 50, loss: 3.34232, mean energy: -14.58271+0.17713j, varE: 5.62505\n",
      "step: 60, loss: -0.82777, mean energy: -15.45306+0.01563j, varE: 6.77785\n",
      "step: 70, loss: 2.34790, mean energy: -16.64071-0.18790j, varE: 3.77840\n",
      "step: 80, loss: 2.29428, mean energy: -16.63818-0.13210j, varE: 2.08316\n",
      "step: 90, loss: 3.44128, mean energy: -16.73352+0.28136j, varE: 4.86252\n",
      "step: 100, loss: 6.12521, mean energy: -17.62121-0.12376j, varE: 2.24605\n",
      "step: 110, loss: 0.69685, mean energy: -17.65031+0.03554j, varE: 3.08913\n",
      "step: 120, loss: 4.11014, mean energy: -17.81325-0.08980j, varE: 1.39905\n",
      "Best model saved at epoch 127 with best E=-18.20065+0.19124j, varE=0.63211\n",
      "step: 130, loss: -1.58212, mean energy: -18.29789-0.06037j, varE: 2.77176\n",
      "step: 140, loss: 2.87111, mean energy: -17.91987-0.03611j, varE: 1.31879\n",
      "step: 150, loss: 2.81795, mean energy: -17.83969+0.00452j, varE: 2.79250\n",
      "step: 160, loss: -1.87030, mean energy: -18.38709-0.13228j, varE: 2.06284\n",
      "step: 170, loss: -0.61212, mean energy: -18.39054-0.23140j, varE: 3.64838\n",
      "step: 180, loss: -4.48755, mean energy: -18.70092-0.01295j, varE: 1.38181\n",
      "Best model saved at epoch 188 with best E=-18.92652+0.03977j, varE=0.95551\n",
      "step: 190, loss: -4.05133, mean energy: -18.77281+0.01538j, varE: 1.20492\n",
      "Best model saved at epoch 191 with best E=-19.01641-0.03313j, varE=0.79521\n",
      "Best model saved at epoch 192 with best E=-19.23736+0.08825j, varE=0.68142\n",
      "step: 200, loss: -2.89813, mean energy: -19.03589-0.05914j, varE: 0.93656\n",
      "Best model saved at epoch 204 with best E=-19.27462-0.02288j, varE=0.51098\n",
      "step: 210, loss: -1.22052, mean energy: -19.18982+0.02986j, varE: 1.45905\n",
      "Best model saved at epoch 211 with best E=-19.29597+0.07676j, varE=0.68271\n",
      "step: 220, loss: -1.55106, mean energy: -19.56943+0.23283j, varE: 9.08423\n",
      "Best model saved at epoch 222 with best E=-19.33275+0.00498j, varE=0.27859\n",
      "Best model saved at epoch 226 with best E=-19.35389-0.02993j, varE=0.30452\n",
      "Best model saved at epoch 228 with best E=-19.44073+0.01851j, varE=0.33809\n",
      "step: 230, loss: 0.21518, mean energy: -19.33651-0.04919j, varE: 0.53526\n",
      "step: 240, loss: -0.67392, mean energy: -19.19124-0.00944j, varE: 0.36738\n",
      "Best model saved at epoch 245 with best E=-19.44312-0.03662j, varE=0.73353\n",
      "step: 250, loss: -0.30957, mean energy: -19.39485+0.02441j, varE: 0.22568\n",
      "step: 260, loss: -4.98071, mean energy: -19.97482+0.33793j, varE: 24.08191\n",
      "step: 270, loss: -1.36615, mean energy: -19.33531+0.06450j, varE: 0.51415\n",
      "step: 280, loss: 3.24863, mean energy: -19.12537+0.14963j, varE: 0.27072\n",
      "step: 290, loss: 4.46805, mean energy: -19.16160+0.07727j, varE: 0.27665\n",
      "step: 300, loss: 1.12744, mean energy: -19.22048-0.05286j, varE: 0.69309\n",
      "Best model saved at epoch 309 with best E=-19.44828-0.01194j, varE=0.24129\n",
      "step: 310, loss: -0.60471, mean energy: -19.37972-0.02610j, varE: 0.22768\n",
      "step: 320, loss: -2.10983, mean energy: -19.16772-0.00564j, varE: 0.74134\n",
      "step: 330, loss: 0.42993, mean energy: -19.35207-0.00796j, varE: 0.25192\n",
      "step: 340, loss: -0.43198, mean energy: -19.25992+0.01704j, varE: 0.36908\n",
      "step: 350, loss: 0.56177, mean energy: -19.36610+0.01589j, varE: 0.12242\n",
      "Best model saved at epoch 358 with best E=-19.48711+0.01512j, varE=0.16143\n",
      "step: 360, loss: 2.17825, mean energy: -19.26148+0.00404j, varE: 0.13839\n",
      "step: 370, loss: 0.73642, mean energy: -19.41005+0.04229j, varE: 0.11671\n",
      "step: 380, loss: -0.13708, mean energy: -19.47755-0.00356j, varE: 0.08473\n",
      "step: 390, loss: 2.39267, mean energy: -19.40519-0.03721j, varE: 0.10268\n",
      "Best model saved at epoch 394 with best E=-19.48749-0.02779j, varE=0.39054\n",
      "Best model saved at epoch 399 with best E=-19.49932+0.01838j, varE=0.13201\n",
      "step: 400, loss: 0.06601, mean energy: -19.38874-0.00894j, varE: 0.08551\n",
      "step: 410, loss: -0.92560, mean energy: -19.43007+0.00572j, varE: 0.26207\n",
      "Best model saved at epoch 414 with best E=-19.51768-0.01299j, varE=0.09957\n",
      "step: 420, loss: 0.49054, mean energy: -19.35464+0.01638j, varE: 0.09604\n",
      "step: 430, loss: 0.03329, mean energy: -19.41612+0.00758j, varE: 0.08258\n",
      "step: 440, loss: 0.37482, mean energy: -19.43823-0.01331j, varE: 0.09306\n",
      "step: 450, loss: -0.54376, mean energy: -19.35491+0.00108j, varE: 0.35979\n",
      "Total time taken: 6.167\n"
     ]
    }
   ],
   "source": [
    "# WITH GRAD CLIPPING (GLOBAL NORM = 8.0)\n",
    "nsteps = 451\n",
    "start = time.time()\n",
    "\n",
    "mE, vE = run_J1J2(wf=wf_egru, numsteps=nsteps, systemsize=syssize, var_tol=var_tol, J1_  = J1, \n",
    "                   J2_ = J2, Marshall_sign = True, \n",
    "                  numsamples = nssamples, learningrate = 1e-2, seed = 111, fname = '../gn_results_grad_clipping_global_norm')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587abf4",
   "metadata": {},
   "source": [
    "# HypGRU\n",
    "- With no gradient clipping, there is a small kink in the energy curve before convergence is reached\n",
    "- With gradient clipping by value in the range [-1,1] and by global norm of 8.0, convergence at the right value could not be reached\n",
    "- With gradient clipping by global norm of 9.0, the kink is eliminated and convergence is reached at near the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d7e7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<j1j2_hyprnn_wf.rnn_hyp_wf at 0x10f53f890>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type = 'HypGRU'\n",
    "hidden_units = 75\n",
    "wf_hgru = rnn_hyp_wf(syssize, cell_type, 'hyp', 'id', hidden_units)\n",
    "wf_hgru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20596a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -0.92465, mean energy: 12.68636+0.32971j, varE: 3.82003\n",
      "step: 10, loss: 3.08801, mean energy: -2.07289-0.03995j, varE: 10.76103\n",
      "step: 20, loss: 7.24075, mean energy: -4.85589+0.18382j, varE: 10.33183\n",
      "step: 30, loss: -0.66522, mean energy: -9.19421+0.08008j, varE: 8.62918\n",
      "step: 40, loss: -2.61273, mean energy: -12.04500+0.39547j, varE: 8.87465\n",
      "step: 50, loss: -7.09064, mean energy: -14.25949-0.01635j, varE: 7.86061\n",
      "step: 60, loss: -2.34944, mean energy: -16.53273-0.13804j, varE: 3.29932\n",
      "step: 70, loss: 2.31102, mean energy: -17.25464+0.05029j, varE: 3.49061\n",
      "step: 80, loss: -0.07616, mean energy: -16.96469-0.15660j, varE: 2.76358\n",
      "Best model saved at epoch 81 with best E=-17.33574-0.23736j, varE=1.62055\n",
      "Best model saved at epoch 83 with best E=-17.46841-0.00353j, varE=1.74533\n",
      "Best model saved at epoch 87 with best E=-17.57648-0.15065j, varE=1.70142\n",
      "step: 90, loss: 0.25479, mean energy: -16.86822+0.06549j, varE: 4.29619\n",
      "step: 100, loss: 1.20917, mean energy: -17.87919-0.17695j, varE: 2.37398\n",
      "Best model saved at epoch 104 with best E=-17.87907-0.10171j, varE=1.94722\n",
      "step: 110, loss: 7.89268, mean energy: -17.39849-0.16411j, varE: 3.31235\n",
      "Best model saved at epoch 112 with best E=-17.88712-0.02131j, varE=1.22787\n",
      "Best model saved at epoch 113 with best E=-17.97904+0.04168j, varE=1.83062\n",
      "Best model saved at epoch 119 with best E=-18.25992+0.21076j, varE=1.28271\n",
      "step: 120, loss: -3.49448, mean energy: -18.12072+0.17382j, varE: 3.22892\n",
      "step: 130, loss: 14.70607, mean energy: -17.65767-0.17627j, varE: 5.11775\n",
      "step: 140, loss: 5.65659, mean energy: -16.36226+0.12552j, varE: 6.87862\n",
      "step: 150, loss: 8.61734, mean energy: -18.06294+0.21153j, varE: 3.81493\n",
      "step: 160, loss: -7.77975, mean energy: -18.19849+0.29087j, varE: 3.19654\n",
      "Best model saved at epoch 161 with best E=-18.54389-0.08795j, varE=1.84876\n",
      "step: 170, loss: 3.05597, mean energy: -18.31037-0.11757j, varE: 2.97198\n",
      "Best model saved at epoch 176 with best E=-19.00881-0.11464j, varE=1.34138\n",
      "step: 180, loss: -1.77917, mean energy: -18.52098-0.04768j, varE: 4.10185\n",
      "step: 190, loss: -1.95621, mean energy: -17.84248-0.21098j, varE: 4.19150\n",
      "step: 200, loss: -2.44156, mean energy: -17.78568+0.03954j, varE: 4.06450\n",
      "step: 210, loss: -4.93494, mean energy: -18.61023-0.02730j, varE: 1.26518\n",
      "Best model saved at epoch 215 with best E=-19.18192-0.01327j, varE=1.83843\n",
      "Best model saved at epoch 216 with best E=-19.19365-0.04862j, varE=1.44591\n",
      "step: 220, loss: -0.32846, mean energy: -18.94034+0.01188j, varE: 2.31551\n",
      "step: 230, loss: -6.30466, mean energy: -19.00574+0.01887j, varE: 1.62974\n",
      "Best model saved at epoch 235 with best E=-19.20728-0.00035j, varE=1.40630\n",
      "Best model saved at epoch 238 with best E=-19.42717+0.08037j, varE=0.84947\n",
      "step: 240, loss: 2.68246, mean energy: -19.16012+0.06389j, varE: 2.06770\n",
      "Best model saved at epoch 243 with best E=-19.48997+0.04812j, varE=1.45793\n",
      "step: 250, loss: -6.45230, mean energy: -19.13725+0.16575j, varE: 1.47246\n",
      "Best model saved at epoch 254 with best E=-19.50156-0.04539j, varE=1.32480\n",
      "Best model saved at epoch 255 with best E=-19.70338-0.10227j, varE=0.50842\n",
      "step: 260, loss: -3.51880, mean energy: -19.66994-0.02349j, varE: 0.82182\n",
      "Best model saved at epoch 263 with best E=-19.75569-0.03508j, varE=0.69372\n",
      "step: 270, loss: -1.72604, mean energy: -19.32200+0.11253j, varE: 1.44759\n",
      "step: 280, loss: 0.22818, mean energy: -19.72956+0.07465j, varE: 0.70173\n",
      "Best model saved at epoch 281 with best E=-19.86645-0.10397j, varE=0.60142\n",
      "step: 290, loss: -1.09637, mean energy: -19.53528+0.02725j, varE: 1.39042\n",
      "Best model saved at epoch 296 with best E=-19.90343-0.04977j, varE=0.41095\n",
      "step: 300, loss: -0.80859, mean energy: -19.60864+0.07798j, varE: 1.14655\n",
      "Best model saved at epoch 303 with best E=-19.91172+0.07475j, varE=0.51496\n",
      "step: 310, loss: -0.07764, mean energy: -19.81946+0.00775j, varE: 0.53851\n",
      "Best model saved at epoch 316 with best E=-19.92984-0.09908j, varE=0.42850\n",
      "step: 320, loss: -3.65555, mean energy: -19.82271-0.01860j, varE: 0.73605\n",
      "step: 330, loss: -2.51950, mean energy: -19.43031+0.03470j, varE: 1.07615\n",
      "step: 340, loss: -1.25992, mean energy: -19.28485+0.02136j, varE: 1.62891\n",
      "step: 350, loss: 1.89679, mean energy: -19.53694-0.01848j, varE: 0.79382\n",
      "step: 360, loss: 5.62946, mean energy: -19.38910-0.06966j, varE: 1.82423\n",
      "step: 370, loss: 0.68243, mean energy: -19.73218-0.03398j, varE: 0.53341\n",
      "Best model saved at epoch 376 with best E=-19.95603+0.07118j, varE=0.55649\n",
      "step: 380, loss: 2.58365, mean energy: -19.76499-0.04060j, varE: 1.20703\n",
      "step: 390, loss: 1.93713, mean energy: -19.87104-0.00626j, varE: 0.54543\n",
      "step: 400, loss: -3.90961, mean energy: -19.62884+0.01975j, varE: 0.75376\n",
      "step: 410, loss: -1.05038, mean energy: -19.80769+0.00064j, varE: 0.31029\n",
      "step: 420, loss: 2.18298, mean energy: -19.94925+0.03148j, varE: 0.34443\n",
      "Best model saved at epoch 429 with best E=-19.95994-0.07894j, varE=0.44844\n",
      "Best model saved at epoch 430 with best E=-20.01546+0.03794j, varE=0.32122\n",
      "step: 430, loss: 0.56226, mean energy: -20.01546+0.03794j, varE: 0.32122\n",
      "step: 440, loss: 0.12997, mean energy: -19.87028+0.03065j, varE: 0.52345\n",
      "Best model saved at epoch 447 with best E=-20.04546-0.00994j, varE=0.22122\n",
      "step: 450, loss: 1.66763, mean energy: -19.95958+0.07230j, varE: 0.83850\n",
      "Total time taken: 41.262\n"
     ]
    }
   ],
   "source": [
    "#gn_results_grad_clipping_global_norm of 9\n",
    "# WITH GRAD CLIPPING\n",
    "nsteps=451\n",
    "start = time.time()\n",
    "mE, vE = run_J1J2_hypvars(wf=wf_hgru, numsteps=nsteps, systemsize=syssize, var_tol=var_tol,\n",
    "                          J1_ = J1, J2_ = J2, Marshall_sign = True, \n",
    "                           numsamples = nssamples,  lr1=1e-2, lr2=1e-2, seed = 111, fname = '../gn_results_grad_clipping_global_norm')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
