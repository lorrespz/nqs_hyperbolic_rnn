{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eac4163-9c22-4ad0-959d-d926b3dacd72",
   "metadata": {},
   "source": [
    "# 1D J1=1.0, J2=0.5: Training with gradient clipping\n",
    "\n",
    "This notebook is part of the work arXiv:2505.22083 (https://arxiv.org/abs/2505.22083), \"Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz\". Code written by HLD. \n",
    "\n",
    "In this notebook, we collected the best training results from training with gradient clipping by value and by global norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9abff75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../utility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fd866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_exact = -18.75\n",
    "syssize = 50\n",
    "nssamples = 50\n",
    "J1 = 1.0\n",
    "J2 = 0.5\n",
    "nsteps = 401\n",
    "var_tol = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2f89e",
   "metadata": {},
   "source": [
    "# EuclGRU\n",
    "- The energy curve with no gradient clipping showed multiple kinks and converged at a high value far from the correct value.\n",
    "- Gradient clipping by value of [-1,1] eliminated the kinks and improved the convergence to the correct value.\n",
    "-  No gradient clipping by global norm was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b34c24ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 16:38:44.567611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with gradient clipping in the range [-1.0,1.0] (both Euclidean and hyperbolic)\n",
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<j1j2_hyprnn_wf.rnn_eucl_wf at 0x19a6e5dd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from j1j2_hyprnn_train_loop_grad_clipping import *\n",
    "cell_type = 'EuclGRU'\n",
    "hidden_units = 75\n",
    "wf_egru = rnn_eucl_wf(syssize, cell_type, hidden_units)\n",
    "wf_egru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b7ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -1.05695, mean energy: 18.03808+0.09419j, varE: 0.64345\n",
      "step: 10, loss: 13.47987, mean energy: -3.38808-0.27251j, varE: 13.48253\n",
      "step: 20, loss: 6.60693, mean energy: -8.06062+0.16802j, varE: 6.12464\n",
      "step: 30, loss: -1.91852, mean energy: -10.75037+0.11048j, varE: 4.17511\n",
      "step: 40, loss: -2.93944, mean energy: -12.41296+0.09308j, varE: 5.97954\n",
      "step: 50, loss: -5.46370, mean energy: -11.79622+0.18810j, varE: 4.25440\n",
      "step: 60, loss: -3.16319, mean energy: -12.91744+0.00946j, varE: 5.26457\n",
      "step: 70, loss: -0.61833, mean energy: -13.43084+0.12261j, varE: 3.00497\n",
      "step: 80, loss: 6.42728, mean energy: -13.94592-0.47673j, varE: 3.41955\n",
      "step: 90, loss: 2.85764, mean energy: -14.56147-0.03251j, varE: 5.14067\n",
      "step: 100, loss: 11.43860, mean energy: -14.77916+0.45880j, varE: 6.21000\n",
      "step: 110, loss: -0.65875, mean energy: -15.89623-0.20456j, varE: 4.75950\n",
      "step: 120, loss: -1.23068, mean energy: -16.21000+0.30454j, varE: 4.01557\n",
      "step: 130, loss: -3.93219, mean energy: -16.91632+0.02004j, varE: 3.41263\n",
      "step: 140, loss: 7.35861, mean energy: -16.25849+0.05132j, varE: 5.12558\n",
      "step: 150, loss: -3.43787, mean energy: -16.41594+0.06760j, varE: 3.30835\n",
      "step: 160, loss: -0.44138, mean energy: -17.04801+0.04573j, varE: 3.03973\n",
      "step: 170, loss: 1.67130, mean energy: -17.12239+0.26361j, varE: 2.79514\n",
      "step: 180, loss: -1.55255, mean energy: -17.24734-0.26766j, varE: 3.87366\n",
      "step: 190, loss: 2.42834, mean energy: -17.47720+0.00953j, varE: 2.05830\n",
      "step: 200, loss: 0.65005, mean energy: -17.89812-0.11161j, varE: 1.78802\n",
      "step: 210, loss: -1.25388, mean energy: -17.52459-0.16061j, varE: 2.67947\n",
      "Best model saved at epoch 213 with best E=-18.33751-0.03856j, varE=0.80421\n",
      "Best model saved at epoch 218 with best E=-18.50660-0.07500j, varE=0.97411\n",
      "step: 220, loss: -5.44254, mean energy: -17.89945+0.08423j, varE: 3.41006\n",
      "step: 230, loss: -1.16772, mean energy: -18.47997+0.04221j, varE: 0.43437\n",
      "Best model saved at epoch 232 with best E=-18.53659-0.02566j, varE=0.65816\n",
      "Best model saved at epoch 235 with best E=-18.58488-0.06091j, varE=0.44282\n",
      "Best model saved at epoch 238 with best E=-18.61110+0.07180j, varE=0.18958\n",
      "step: 240, loss: -0.78940, mean energy: -18.36013+0.04533j, varE: 0.97091\n",
      "Best model saved at epoch 243 with best E=-18.62084+0.01139j, varE=0.27807\n",
      "Best model saved at epoch 244 with best E=-18.63616-0.00158j, varE=0.23185\n",
      "Best model saved at epoch 248 with best E=-18.63958+0.01736j, varE=0.38063\n",
      "step: 250, loss: 0.65637, mean energy: -18.62304-0.03434j, varE: 0.25571\n",
      "Best model saved at epoch 251 with best E=-18.64542-0.01012j, varE=0.16553\n",
      "Best model saved at epoch 252 with best E=-18.67663-0.00813j, varE=0.63543\n",
      "Best model saved at epoch 253 with best E=-18.72493-0.00026j, varE=0.20685\n",
      "step: 260, loss: -2.06619, mean energy: -18.55806+0.04134j, varE: 0.64013\n",
      "step: 270, loss: -1.63840, mean energy: -18.12079-0.04824j, varE: 2.06684\n",
      "step: 280, loss: -3.10834, mean energy: -18.41149-0.10304j, varE: 1.09626\n",
      "step: 290, loss: 0.31519, mean energy: -18.60359-0.01922j, varE: 0.57417\n",
      "step: 300, loss: 1.74792, mean energy: -18.61559+0.02674j, varE: 0.49270\n",
      "step: 310, loss: 0.48465, mean energy: -18.40711-0.00265j, varE: 0.34568\n",
      "Best model saved at epoch 316 with best E=-18.73232-0.08923j, varE=0.78429\n",
      "step: 320, loss: 0.68176, mean energy: -18.48590-0.01075j, varE: 0.43384\n",
      "step: 330, loss: -0.45187, mean energy: -18.66376+0.00504j, varE: 0.05846\n",
      "step: 340, loss: 0.02399, mean energy: -18.27626-0.07955j, varE: 0.56604\n",
      "step: 350, loss: -1.87424, mean energy: -18.44358-0.03026j, varE: 0.43844\n",
      "step: 360, loss: -1.61578, mean energy: -18.66181-0.02413j, varE: 0.09065\n",
      "Best model saved at epoch 361 with best E=-18.73687-0.02116j, varE=0.11905\n",
      "Best model saved at epoch 365 with best E=-18.82428-0.01731j, varE=0.09320\n",
      "step: 370, loss: -0.76605, mean energy: -18.60877-0.01076j, varE: 0.67233\n",
      "step: 380, loss: -0.43201, mean energy: -18.65604+0.01198j, varE: 0.17512\n",
      "step: 390, loss: 0.07526, mean energy: -18.71803+0.00261j, varE: 0.07238\n",
      "step: 400, loss: -0.03790, mean energy: -18.70893-0.01020j, varE: 0.02101\n",
      "step: 410, loss: -0.26312, mean energy: -18.74370-0.01610j, varE: 0.02214\n",
      "step: 420, loss: 0.20447, mean energy: -18.76023+0.00448j, varE: 0.07372\n",
      "step: 430, loss: -0.11438, mean energy: -18.71371-0.00006j, varE: 0.04102\n",
      "step: 440, loss: -0.24677, mean energy: -18.61384+0.02178j, varE: 0.27789\n",
      "step: 450, loss: -1.72702, mean energy: -18.62250+0.00193j, varE: 0.34015\n",
      "Total time taken: 5.929\n"
     ]
    }
   ],
   "source": [
    "#WITH GRAD CLIPPING [-1,1]\n",
    "nsteps = 451\n",
    "start = time.time()\n",
    "\n",
    "mE, vE = run_J1J2(wf=wf_egru, numsteps=nsteps, systemsize=syssize, var_tol= 1.0, J1_  = J1, \n",
    "                   J2_ = J2, Marshall_sign = True, \n",
    "                  numsamples = nssamples, learningrate = 1e-2, seed = 111, fname = '../results_grad_clipping')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587abf4",
   "metadata": {},
   "source": [
    "# HypGRU\n",
    "- NO gradient clipping was performed since the original training showed a smooth curve that converged at the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b991462c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<j1j2_hyprnn_wf.rnn_hyp_wf at 0x1a104eb90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NO GRAD CLIPPING\n",
    "from j1j2_hyprnn_train_loop import *\n",
    "cell_type = 'HypGRU'\n",
    "hidden_units = 70\n",
    "wf_hgru = rnn_hyp_wf(syssize, cell_type, 'hyp', 'id', hidden_units)\n",
    "wf_hgru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659efb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -2.03650, mean energy: 17.50668-0.35098j, varE: 1.58106\n",
      "step: 10, loss: 1.77698, mean energy: -2.41737+0.21426j, varE: 10.51818\n",
      "step: 20, loss: -0.17918, mean energy: -3.62105+0.00802j, varE: 13.37337\n",
      "step: 30, loss: -0.57536, mean energy: -6.97623+0.04984j, varE: 9.35391\n",
      "step: 40, loss: -0.86969, mean energy: -7.91884+0.01327j, varE: 8.56101\n",
      "step: 50, loss: -13.84029, mean energy: -9.77025-0.77116j, varE: 10.29447\n",
      "step: 60, loss: 0.27380, mean energy: -11.32603-0.25297j, varE: 6.58634\n",
      "step: 70, loss: -3.91599, mean energy: -13.35884+0.24383j, varE: 5.84760\n",
      "step: 80, loss: -3.76614, mean energy: -13.64775-0.25852j, varE: 3.91111\n",
      "step: 90, loss: 5.40735, mean energy: -14.65660+0.13743j, varE: 6.07031\n",
      "step: 100, loss: -13.16647, mean energy: -15.23525+0.03523j, varE: 5.97112\n",
      "step: 110, loss: 1.22281, mean energy: -16.09338+0.12285j, varE: 3.08350\n",
      "step: 120, loss: 3.10577, mean energy: -16.17319-0.10365j, varE: 4.42243\n",
      "step: 130, loss: -4.07657, mean energy: -16.82370-0.00492j, varE: 4.61847\n",
      "step: 140, loss: -10.04300, mean energy: -16.27476-0.00046j, varE: 3.34031\n",
      "Best model saved at epoch 143 with best E=-16.88605+0.21995j, varE=1.94626\n",
      "step: 150, loss: -5.89630, mean energy: -16.79513-0.17855j, varE: 2.66819\n",
      "Best model saved at epoch 151 with best E=-17.23630-0.20274j, varE=1.61291\n",
      "Best model saved at epoch 157 with best E=-17.45771-0.04765j, varE=1.41345\n",
      "step: 160, loss: 2.57135, mean energy: -16.79129+0.20799j, varE: 3.76865\n",
      "Best model saved at epoch 166 with best E=-17.49560+0.03049j, varE=1.79200\n",
      "Best model saved at epoch 170 with best E=-17.52229+0.06756j, varE=1.31739\n",
      "step: 170, loss: 2.38815, mean energy: -17.52229+0.06756j, varE: 1.31739\n",
      "Best model saved at epoch 179 with best E=-17.53842+0.08535j, varE=1.41112\n",
      "step: 180, loss: -7.61255, mean energy: -17.34175+0.00443j, varE: 1.87022\n",
      "Best model saved at epoch 182 with best E=-17.68279+0.05336j, varE=1.14200\n",
      "step: 190, loss: 9.64261, mean energy: -17.58114-0.04433j, varE: 2.06006\n",
      "step: 200, loss: -2.17801, mean energy: -17.56441-0.12857j, varE: 1.80016\n",
      "Best model saved at epoch 203 with best E=-17.84169+0.04530j, varE=1.14986\n",
      "step: 210, loss: -5.89438, mean energy: -17.55374-0.06281j, varE: 1.96060\n",
      "Best model saved at epoch 215 with best E=-18.02596+0.21653j, varE=0.82138\n",
      "step: 220, loss: -1.95114, mean energy: -17.77372+0.07649j, varE: 2.09094\n",
      "Best model saved at epoch 226 with best E=-18.03531+0.29324j, varE=1.15951\n",
      "Best model saved at epoch 227 with best E=-18.07759-0.13499j, varE=0.89703\n",
      "step: 230, loss: 2.90573, mean energy: -17.80727+0.28645j, varE: 1.36196\n",
      "Best model saved at epoch 235 with best E=-18.13347-0.08622j, varE=1.15613\n",
      "Best model saved at epoch 237 with best E=-18.19191-0.08389j, varE=0.82417\n",
      "Best model saved at epoch 238 with best E=-18.29785-0.16838j, varE=0.74070\n",
      "step: 240, loss: -2.10110, mean energy: -18.06454+0.00742j, varE: 1.25102\n",
      "Best model saved at epoch 241 with best E=-18.31530-0.14094j, varE=0.76399\n",
      "Best model saved at epoch 247 with best E=-18.35728-0.05725j, varE=1.30209\n",
      "step: 250, loss: -0.57065, mean energy: -18.32133+0.19630j, varE: 0.95572\n",
      "Best model saved at epoch 254 with best E=-18.44555-0.02783j, varE=0.61919\n",
      "Best model saved at epoch 258 with best E=-18.54344-0.14751j, varE=0.93555\n",
      "step: 260, loss: -1.67966, mean energy: -18.25119-0.04570j, varE: 1.66349\n",
      "step: 270, loss: -1.43951, mean energy: -18.23578-0.09761j, varE: 1.15522\n",
      "Best model saved at epoch 272 with best E=-18.56540-0.06866j, varE=0.62729\n",
      "step: 280, loss: -1.57550, mean energy: -18.53835+0.05901j, varE: 0.51556\n",
      "step: 290, loss: -0.72693, mean energy: -18.56485-0.07035j, varE: 0.43935\n",
      "step: 300, loss: -3.65851, mean energy: -18.45297+0.07014j, varE: 0.64597\n",
      "Best model saved at epoch 305 with best E=-18.62821-0.01600j, varE=0.26703\n",
      "step: 310, loss: -0.38965, mean energy: -18.58971+0.00691j, varE: 0.23456\n",
      "Best model saved at epoch 317 with best E=-18.66438+0.01752j, varE=0.28287\n",
      "Best model saved at epoch 318 with best E=-18.72189-0.01043j, varE=0.20868\n",
      "step: 320, loss: -0.15143, mean energy: -18.47968+0.07090j, varE: 0.71873\n",
      "step: 330, loss: 4.43933, mean energy: -18.32482-0.02603j, varE: 0.55315\n",
      "Best model saved at epoch 339 with best E=-18.72438+0.15829j, varE=0.89745\n",
      "step: 340, loss: 3.59192, mean energy: -18.50939+0.13229j, varE: 0.50652\n",
      "step: 350, loss: 0.59171, mean energy: -18.53131+0.01524j, varE: 0.31588\n",
      "step: 360, loss: 0.16364, mean energy: -18.65597+0.08318j, varE: 0.33736\n",
      "step: 370, loss: 1.73987, mean energy: -18.56543-0.06336j, varE: 0.42408\n",
      "step: 380, loss: 4.02939, mean energy: -18.59795-0.10414j, varE: 0.61913\n",
      "Best model saved at epoch 387 with best E=-18.72637-0.05697j, varE=0.22732\n",
      "step: 390, loss: 0.41074, mean energy: -18.68452-0.02032j, varE: 0.17299\n",
      "step: 400, loss: -0.77838, mean energy: -18.71954+0.03028j, varE: 0.06471\n",
      "Best model saved at epoch 404 with best E=-18.75780-0.02896j, varE=0.06536\n",
      "step: 410, loss: -1.69693, mean energy: -18.66464+0.03127j, varE: 0.13810\n",
      "step: 420, loss: -0.40280, mean energy: -18.67433-0.04168j, varE: 0.13648\n",
      "step: 430, loss: 0.20499, mean energy: -18.68986-0.00231j, varE: 0.15570\n",
      "step: 440, loss: 0.35461, mean energy: -18.58928-0.06559j, varE: 0.79966\n",
      "step: 450, loss: 0.83374, mean energy: -18.47594+0.06079j, varE: 0.46154\n",
      "Total time taken: 165683.04630088806\n"
     ]
    }
   ],
   "source": [
    "nsteps=451\n",
    "start = time.time()\n",
    "mE, vE = run_J1J2_hypvars(wf=wf_hgru, numsteps=nsteps, systemsize=syssize, var_tol=2.0,\n",
    "                          J1_ = J1, J2_ = J2, Marshall_sign = True, \n",
    "                           numsamples = nssamples,  lr1=1e-2, lr2=1e-2, seed = 111, fname = '../results')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {duration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7644b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken in hours is 46.023\n"
     ]
    }
   ],
   "source": [
    "print(f'Total time taken in hours is {np.round(165683.04630088806/3600,3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
