{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8299aa2d-f67c-4021-bbf8-1f57c5681ea3",
   "metadata": {},
   "source": [
    "# 1D J1=1.0, J2=0.0: Training with gradient clipping\n",
    "\n",
    "This notebook is part of the work arXiv:2505.22083 (https://arxiv.org/abs/2505.22083), \"Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz\". Code written by HLD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9abff75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 18:41:40.004661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with gradient clipping by global norm of 8.0 (both Euclidean and hyperbolic)\n",
      "The norm clipping is defined in the Adam optimizer\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../utility')\n",
    "from j1j2_hyprnn_train_loop_grad_clipping_gn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fd866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_exact = -21.9721\n",
    "syssize = 50\n",
    "nssamples = 50\n",
    "J1 = 1.0\n",
    "J2 = 0.0\n",
    "var_tol = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2f89e",
   "metadata": {},
   "source": [
    "# EuclGRU\n",
    "- The no clipping case has a kink in the energy convergence curve.\n",
    "- Clipping by value in the range [-1,1] eliminated the kink but led to a worse result in the no clipping case.\n",
    "- Clipping with global norm of 8.0 eliminated the kink and led to a better result than the no clipping case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e2e873f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<j1j2_hyprnn_wf.rnn_eucl_wf at 0x13d2dce60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type = 'EuclGRU'\n",
    "hidden_units = 75\n",
    "wf_egru = rnn_eucl_wf(syssize, cell_type, hidden_units)\n",
    "wf_egru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6002a5b-59d2-469c-9a30-dbe0dc46a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -0.52756, mean energy: 12.05817+0.07567j, varE: 0.18605\n",
      "step: 10, loss: -3.35701, mean energy: -3.01039+0.11845j, varE: 9.50387\n",
      "step: 20, loss: -1.59732, mean energy: -10.23455-0.00318j, varE: 9.74746\n",
      "step: 30, loss: -4.76671, mean energy: -14.28209-0.06600j, varE: 10.34802\n",
      "step: 40, loss: 4.08353, mean energy: -16.18631-0.16386j, varE: 6.41848\n",
      "step: 50, loss: -0.08484, mean energy: -16.90627-0.07891j, varE: 7.01464\n",
      "step: 60, loss: 2.18680, mean energy: -16.70524-0.00042j, varE: 4.39975\n",
      "step: 70, loss: 1.32184, mean energy: -17.56263-0.11947j, varE: 6.37796\n",
      "step: 80, loss: 3.11919, mean energy: -19.20714+0.00435j, varE: 3.41796\n",
      "step: 90, loss: 6.82774, mean energy: -19.21720-0.13891j, varE: 3.63528\n",
      "step: 100, loss: -4.40378, mean energy: -19.07112+0.19135j, varE: 3.84235\n",
      "step: 110, loss: 8.89722, mean energy: -19.16039-0.00685j, varE: 5.30370\n",
      "step: 120, loss: -3.08514, mean energy: -19.60507+0.00065j, varE: 2.43396\n",
      "step: 130, loss: 12.27612, mean energy: -19.65952-0.03777j, varE: 4.08779\n",
      "step: 140, loss: -10.54742, mean energy: -19.63340+0.25363j, varE: 4.60283\n",
      "Best model saved at epoch 144 with best E=-20.56164+0.11929j, varE=0.78098\n",
      "step: 150, loss: -0.79277, mean energy: -20.40088-0.19854j, varE: 4.20723\n",
      "step: 160, loss: 1.05609, mean energy: -20.67556+0.11013j, varE: 2.11809\n",
      "Best model saved at epoch 161 with best E=-20.89102+0.03177j, varE=0.68234\n",
      "step: 170, loss: 0.77859, mean energy: -21.00311-0.13869j, varE: 1.89967\n",
      "Best model saved at epoch 175 with best E=-21.18603+0.01059j, varE=0.60793\n",
      "step: 180, loss: -2.96243, mean energy: -20.94653-0.04122j, varE: 0.50536\n",
      "Best model saved at epoch 183 with best E=-21.23603+0.02918j, varE=0.44325\n",
      "step: 190, loss: -2.05756, mean energy: -21.01809-0.01926j, varE: 0.81640\n",
      "step: 200, loss: -0.15555, mean energy: -21.12936+0.02002j, varE: 0.20058\n",
      "Best model saved at epoch 204 with best E=-21.24718+0.02416j, varE=0.56200\n",
      "Best model saved at epoch 207 with best E=-21.28806-0.08489j, varE=0.46178\n",
      "step: 210, loss: -0.40738, mean energy: -21.05125-0.06709j, varE: 0.85640\n",
      "step: 220, loss: 0.90506, mean energy: -21.10155+0.04255j, varE: 0.83275\n",
      "step: 230, loss: 0.89227, mean energy: -21.25061-0.07470j, varE: 0.99375\n",
      "step: 240, loss: 1.78116, mean energy: -21.16318-0.00494j, varE: 0.74184\n",
      "step: 250, loss: -0.15463, mean energy: -21.15241+0.03607j, varE: 0.69412\n",
      "step: 260, loss: -0.17493, mean energy: -21.16125-0.00238j, varE: 0.69000\n",
      "step: 270, loss: 0.93604, mean energy: -21.08333+0.00070j, varE: 0.52739\n",
      "Best model saved at epoch 272 with best E=-21.29260+0.05167j, varE=0.28503\n",
      "Best model saved at epoch 274 with best E=-21.38616-0.02046j, varE=0.43122\n",
      "step: 280, loss: 1.22748, mean energy: -21.21111-0.00556j, varE: 0.28034\n",
      "step: 290, loss: -1.47812, mean energy: -21.29736-0.04364j, varE: 0.37046\n",
      "Best model saved at epoch 292 with best E=-21.38997+0.00930j, varE=0.16571\n",
      "Best model saved at epoch 293 with best E=-21.42986+0.02722j, varE=0.16936\n",
      "step: 300, loss: -1.28745, mean energy: -21.34825-0.03654j, varE: 0.42712\n",
      "step: 310, loss: 2.14505, mean energy: -21.40889-0.00179j, varE: 0.29563\n",
      "step: 320, loss: -1.11884, mean energy: -21.37070-0.07287j, varE: 0.34221\n",
      "Best model saved at epoch 323 with best E=-21.44253-0.01373j, varE=0.18018\n",
      "step: 330, loss: -0.12436, mean energy: -21.38977-0.01170j, varE: 0.34763\n",
      "Best model saved at epoch 337 with best E=-21.44267-0.01616j, varE=0.08390\n",
      "step: 340, loss: 1.51758, mean energy: -21.33734+0.01094j, varE: 0.16412\n",
      "Best model saved at epoch 346 with best E=-21.47392-0.00708j, varE=0.13817\n",
      "step: 350, loss: 1.99704, mean energy: -21.34735-0.01801j, varE: 0.19445\n",
      "step: 360, loss: 1.35358, mean energy: -21.47311+0.00280j, varE: 0.17168\n",
      "Best model saved at epoch 363 with best E=-21.50548-0.07376j, varE=0.72918\n",
      "step: 370, loss: 2.10315, mean energy: -21.27456+0.01372j, varE: 0.40039\n",
      "step: 380, loss: -0.63080, mean energy: -21.29312-0.03825j, varE: 0.21378\n",
      "Best model saved at epoch 384 with best E=-21.59233-0.05065j, varE=0.39674\n",
      "step: 390, loss: 0.39551, mean energy: -21.36649+0.01251j, varE: 0.13561\n",
      "step: 400, loss: -0.34048, mean energy: -21.49462+0.00958j, varE: 0.65154\n",
      "step: 410, loss: -0.37354, mean energy: -21.44787-0.00825j, varE: 0.21588\n",
      "step: 420, loss: -0.58560, mean energy: -21.35918+0.04106j, varE: 0.30503\n",
      "step: 430, loss: 1.34732, mean energy: -21.34067+0.02685j, varE: 0.15600\n",
      "step: 440, loss: -1.12070, mean energy: -21.34905-0.03077j, varE: 0.38757\n",
      "step: 450, loss: -0.12115, mean energy: -21.29549-0.00214j, varE: 0.51695\n",
      "Total time taken: 10.09\n"
     ]
    }
   ],
   "source": [
    "#WIH GRADIENT CLIPPING: GLOBAL NORM 8.0\n",
    "nsteps = 451\n",
    "start = time.time()\n",
    "\n",
    "mE, vE = run_J1J2(wf=wf_egru, numsteps=nsteps, systemsize=syssize, var_tol=0.8, J1_  = J1, \n",
    "                   J2_ = J2, Marshall_sign = True, \n",
    "                  numsamples = nssamples, learningrate = 1e-2, seed = 111, fname = '../gn_results_grad_clipping_global_norm')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587abf4",
   "metadata": {},
   "source": [
    "# HypGRU (units = 60)\n",
    "\n",
    "- The no clipping case has a kink in the energy curve.\n",
    "- Clipping by the global norm of 8.0 led to a much worse results compared to the no gradient clipping case (values converging in the opposite direction, with many instabilities).\n",
    "- Clipping by value in the range of [-1,1] eliminated the kink and led to a better result compared to no gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20eb48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<j1j2_hyprnn_wf.rnn_hyp_wf at 0x1330a2c90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type = 'HypGRU'\n",
    "hidden_units = 60\n",
    "wf_hgru = rnn_hyp_wf(syssize, cell_type, 'hyp', 'id', hidden_units)\n",
    "wf_hgru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58853a-de10-491a-9a14-68f3d27d6665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -0.92902, mean energy: 11.82525-0.35288j, varE: 1.14977\n",
      "step: 10, loss: -6.00800, mean energy: -4.82071+0.66407j, varE: 15.49046\n",
      "step: 20, loss: -5.75761, mean energy: -9.20987-0.02513j, varE: 9.76353\n",
      "step: 30, loss: 6.09009, mean energy: -13.62626+0.01417j, varE: 11.07539\n",
      "step: 40, loss: -4.46379, mean energy: -15.18603+0.02579j, varE: 8.35044\n",
      "step: 50, loss: -2.60687, mean energy: -14.58490-0.20262j, varE: 10.14807\n",
      "step: 60, loss: -10.39885, mean energy: -15.69618-0.21522j, varE: 8.73098\n",
      "step: 70, loss: -8.58556, mean energy: -17.20873-0.21899j, varE: 6.68708\n",
      "step: 80, loss: 11.73067, mean energy: -17.53881-0.16029j, varE: 3.75793\n",
      "step: 90, loss: -3.82687, mean energy: -18.34618-0.17288j, varE: 3.65695\n",
      "step: 100, loss: 0.71140, mean energy: -18.55058+0.12360j, varE: 3.55686\n",
      "step: 110, loss: -1.60905, mean energy: -18.82741-0.04570j, varE: 2.11870\n",
      "Best model saved at epoch 117 with best E=-19.55673+0.00414j, varE=1.41861\n",
      "step: 120, loss: -1.51529, mean energy: -19.23568-0.07022j, varE: 4.16936\n",
      "step: 130, loss: 2.42630, mean energy: -19.90498+0.04788j, varE: 3.01956\n",
      "step: 140, loss: 0.99406, mean energy: -19.43514-0.10368j, varE: 3.11515\n",
      "step: 150, loss: -4.69301, mean energy: -19.77864+0.23824j, varE: 3.30524\n",
      "Best model saved at epoch 152 with best E=-19.76473+0.05463j, varE=1.54862\n",
      "step: 160, loss: -2.96559, mean energy: -19.76945-0.00152j, varE: 4.20851\n",
      "Best model saved at epoch 170 with best E=-20.45528-0.09236j, varE=1.42572\n",
      "step: 170, loss: 1.43054, mean energy: -20.45528-0.09236j, varE: 1.42572\n",
      "step: 180, loss: -1.43024, mean energy: -19.91546+0.20530j, varE: 3.23093\n",
      "step: 190, loss: 2.40353, mean energy: -19.43509-0.10972j, varE: 3.04790\n",
      "step: 200, loss: -11.28293, mean energy: -19.19810-0.10534j, varE: 3.90345\n",
      "step: 210, loss: -15.55113, mean energy: -18.03573+0.04368j, varE: 7.61772\n",
      "step: 220, loss: -5.36636, mean energy: -20.09765-0.30129j, varE: 2.83742\n",
      "step: 230, loss: -10.79108, mean energy: -19.89375-0.14099j, varE: 2.18823\n",
      "Best model saved at epoch 235 with best E=-20.49518-0.10245j, varE=1.65539\n",
      "step: 240, loss: 0.15637, mean energy: -20.33958+0.04354j, varE: 2.28053\n",
      "Best model saved at epoch 241 with best E=-20.50344-0.15096j, varE=1.88398\n",
      "Best model saved at epoch 244 with best E=-20.71888+0.07680j, varE=1.09855\n",
      "Best model saved at epoch 246 with best E=-20.97001+0.05401j, varE=1.58244\n",
      "step: 250, loss: 0.40231, mean energy: -20.79111+0.01071j, varE: 0.99368\n",
      "step: 260, loss: -2.00974, mean energy: -20.52377+0.05019j, varE: 2.28467\n",
      "Best model saved at epoch 266 with best E=-21.04908-0.01479j, varE=1.11376\n",
      "step: 270, loss: 7.20557, mean energy: -20.58283+0.05771j, varE: 1.01917\n",
      "Best model saved at epoch 280 with best E=-21.08537+0.06035j, varE=1.30654\n",
      "step: 280, loss: 0.78720, mean energy: -21.08537+0.06035j, varE: 1.30654\n",
      "step: 290, loss: -3.06064, mean energy: -20.14620-0.19348j, varE: 3.10390\n",
      "step: 300, loss: -1.25427, mean energy: -20.96157+0.10298j, varE: 1.44234\n",
      "step: 310, loss: 0.71204, mean energy: -20.93337-0.07473j, varE: 1.27508\n",
      "step: 320, loss: 2.10400, mean energy: -20.72006+0.03344j, varE: 1.10161\n",
      "step: 330, loss: 12.24954, mean energy: -20.64340+0.33287j, varE: 6.15594\n",
      "step: 340, loss: 5.43567, mean energy: -20.39956+0.07157j, varE: 2.27906\n",
      "step: 350, loss: 4.17023, mean energy: -21.02579+0.01698j, varE: 1.12751\n",
      "step: 360, loss: -2.28757, mean energy: -20.85649-0.02793j, varE: 2.40464\n",
      "step: 370, loss: -0.91916, mean energy: -20.18866+0.05588j, varE: 2.03560\n",
      "step: 380, loss: 8.59525, mean energy: -19.62485+0.27166j, varE: 3.49554\n",
      "step: 390, loss: 4.37357, mean energy: -20.55621+0.04689j, varE: 3.03257\n",
      "step: 400, loss: 0.32727, mean energy: -20.97915+0.00932j, varE: 3.68637\n",
      "Best model saved at epoch 406 with best E=-21.24596-0.03208j, varE=0.72265\n",
      "step: 410, loss: -1.81784, mean energy: -20.84157-0.01457j, varE: 2.84308\n",
      "Best model saved at epoch 415 with best E=-21.32201+0.04306j, varE=0.88428\n",
      "step: 420, loss: -1.31064, mean energy: -21.10495+0.04642j, varE: 1.14793\n",
      "step: 430, loss: -0.47073, mean energy: -20.28106-0.13692j, varE: 3.06890\n"
     ]
    }
   ],
   "source": [
    "#WITH GRAD CLIPPING BY VALUE [-1,1]\n",
    "nsteps=451\n",
    "start = time.time()\n",
    "mE, vE = run_J1J2_hypvars(wf=wf_hgru, numsteps=nsteps, systemsize=syssize, var_tol=var_tol,\n",
    "                          J1_ = J1, J2_ = J2, Marshall_sign = True, \n",
    "                           numsamples = nssamples,  lr1=1e-2, lr2=1e-2, seed = 111, fname = '../results_grad_clipping')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
