{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb653aa-63e9-4bc7-8f2f-5f753a1d9a45",
   "metadata": {},
   "source": [
    "# 2d TFIM, Nx=8, Ny=8: RNN wavefunction\n",
    "\n",
    "This notebook is part of the work arXiv:2505.22083 (https://arxiv.org/abs/2505.22083), \"Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz\". Code written by HLD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7affa059-8897-4797-b886-67902add0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6cd7bf-cb6d-4618-9631-4dc5c6f8313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../utility')\n",
    "nx=8\n",
    "ny=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d87ce-9c95-41c8-8434-35b7bef0ab01",
   "metadata": {},
   "source": [
    "# 1D Euclidean RNN wavefunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d199a7af-a3fe-4c87-9cb1-0f8d6827ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:41:02.358705: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tfim2d_1drnn_train_loop_save_last import run_2DTFIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee654683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -1.01016, mean energy: -194.12011, varE: 105.94376\n",
      "step: 10, loss: -2.05295, mean energy: -199.13126, varE: 47.25114\n",
      "step: 20, loss: -0.38041, mean energy: -197.73236, varE: 53.64661\n",
      "step: 30, loss: 9.02978, mean energy: -197.95509, varE: 39.83900\n",
      "step: 40, loss: -3.72533, mean energy: -198.41809, varE: 32.42116\n",
      "step: 50, loss: 3.65474, mean energy: -198.58553, varE: 26.25221\n",
      "step: 60, loss: 2.53734, mean energy: -199.01711, varE: 32.43492\n",
      "step: 70, loss: -0.47330, mean energy: -201.39761, varE: 29.21680\n",
      "step: 80, loss: -0.57055, mean energy: -200.57304, varE: 29.02843\n",
      "step: 90, loss: -2.04901, mean energy: -199.21501, varE: 29.18807\n",
      "step: 100, loss: 0.31427, mean energy: -201.91199, varE: 20.98724\n",
      "step: 110, loss: -1.46453, mean energy: -200.40039, varE: 27.13085\n",
      "step: 120, loss: 1.88994, mean energy: -200.69461, varE: 28.15621\n",
      "step: 130, loss: 6.71595, mean energy: -200.08911, varE: 33.15025\n",
      "step: 140, loss: -1.13198, mean energy: -200.07227, varE: 27.58354\n",
      "step: 150, loss: 3.49645, mean energy: -201.05484, varE: 21.67925\n",
      "step: 160, loss: 0.16699, mean energy: -202.13432, varE: 17.08291\n",
      "step: 170, loss: 0.42110, mean energy: -201.60291, varE: 23.83809\n",
      "step: 180, loss: 1.73518, mean energy: -201.36248, varE: 24.13209\n",
      "step: 190, loss: -1.70764, mean energy: -201.01533, varE: 25.43859\n",
      "step: 200, loss: -1.63883, mean energy: -200.39367, varE: 19.74297\n",
      "step: 210, loss: -1.62344, mean energy: -200.33341, varE: 22.98134\n",
      "step: 220, loss: -1.40882, mean energy: -199.85039, varE: 26.78947\n",
      "step: 230, loss: 1.24207, mean energy: -200.71435, varE: 25.58574\n",
      "step: 240, loss: 0.47246, mean energy: -200.10128, varE: 9.77464\n",
      "step: 250, loss: -0.40503, mean energy: -201.02806, varE: 16.97381\n",
      "step: 260, loss: -5.81583, mean energy: -199.85660, varE: 33.14658\n",
      "step: 270, loss: -2.74103, mean energy: -200.63259, varE: 27.67870\n",
      "step: 280, loss: -4.03466, mean energy: -200.69195, varE: 19.06689\n",
      "step: 290, loss: 0.78587, mean energy: -201.35523, varE: 14.59408\n",
      "step: 300, loss: -0.87935, mean energy: -199.96922, varE: 33.62298\n",
      "step: 310, loss: -1.57626, mean energy: -199.73812, varE: 30.81852\n",
      "step: 320, loss: -0.55415, mean energy: -201.44014, varE: 15.08169\n",
      "step: 330, loss: 1.42829, mean energy: -201.80003, varE: 13.71501\n",
      "step: 340, loss: 2.20247, mean energy: -200.35679, varE: 14.10193\n",
      "step: 350, loss: 0.12869, mean energy: -200.18210, varE: 18.13864\n",
      "Training took 3.542 hours\n"
     ]
    }
   ],
   "source": [
    "#DMRG results:  -202.5077381\n",
    "t0 = time.time()\n",
    "RNNEnergy, varRNNEnergy = run_2DTFIM(numsteps = 351, Nx = nx, Ny = ny,\n",
    "                                     Bx = 3, cell = 'EuclGRU', num_units = 50, numsamples = 50, \n",
    "                                     learningrate = 1e-2, var_tol=30.0, seed = 111, fname = 'results_nov')\n",
    "t1 = time.time()\n",
    "print(f'Training took {np.round((t1-t0)/3600,3)} hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb291d-b23c-4daf-82c3-6e1e6f852f73",
   "metadata": {},
   "source": [
    "# 1D hyperbolic GRU wavefunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2abed6cf-cc47-40c3-bb56-f6fca9d01d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfim2d_1drnn_train_loop_save_last import run_2DTFIM_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fab61aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -0.84225, mean energy: -190.23266, varE: 94.04581\n",
      "step: 10, loss: -2.14594, mean energy: -199.14990, varE: 75.99336\n",
      "step: 20, loss: -1.36112, mean energy: -198.09132, varE: 55.98674\n",
      "step: 30, loss: 0.08679, mean energy: -197.36827, varE: 51.99441\n",
      "step: 40, loss: -1.50906, mean energy: -198.71536, varE: 32.87601\n",
      "step: 50, loss: 2.90450, mean energy: -199.96219, varE: 36.74387\n",
      "step: 60, loss: 0.22966, mean energy: -199.77490, varE: 35.48899\n",
      "step: 70, loss: 0.31747, mean energy: -200.35084, varE: 43.43557\n",
      "step: 80, loss: 0.64374, mean energy: -199.91087, varE: 37.90001\n",
      "step: 90, loss: -0.32249, mean energy: -200.24192, varE: 24.92246\n",
      "step: 100, loss: -0.80192, mean energy: -203.06410, varE: 26.09190\n",
      "step: 110, loss: -2.60653, mean energy: -200.54142, varE: 21.50521\n",
      "step: 120, loss: 4.72264, mean energy: -199.61763, varE: 34.59264\n",
      "step: 130, loss: -0.14387, mean energy: -201.41986, varE: 13.78462\n",
      "step: 140, loss: -0.61702, mean energy: -199.50320, varE: 25.71687\n",
      "step: 150, loss: 4.26422, mean energy: -200.52687, varE: 18.03066\n",
      "step: 160, loss: -0.66159, mean energy: -201.20460, varE: 24.47153\n",
      "step: 170, loss: 0.45252, mean energy: -201.39835, varE: 24.12998\n",
      "step: 180, loss: -0.27941, mean energy: -201.39476, varE: 24.42448\n",
      "step: 190, loss: -2.25996, mean energy: -201.09457, varE: 23.98447\n",
      "step: 200, loss: -1.94080, mean energy: -200.95481, varE: 19.62456\n",
      "step: 210, loss: -0.92872, mean energy: -200.80002, varE: 14.93538\n",
      "step: 220, loss: -1.01308, mean energy: -200.20437, varE: 18.49835\n",
      "step: 230, loss: -1.32758, mean energy: -201.46048, varE: 17.65597\n",
      "step: 240, loss: 0.28129, mean energy: -200.67927, varE: 11.08069\n",
      "step: 250, loss: -0.36526, mean energy: -201.97829, varE: 16.46939\n",
      "step: 260, loss: -3.15778, mean energy: -201.50180, varE: 13.55289\n",
      "step: 270, loss: -1.53174, mean energy: -201.94901, varE: 9.33710\n",
      "step: 280, loss: 0.88005, mean energy: -201.79920, varE: 14.01820\n",
      "step: 290, loss: 0.86029, mean energy: -201.70429, varE: 12.19841\n",
      "step: 300, loss: -0.21678, mean energy: -201.44744, varE: 10.08311\n",
      "step: 310, loss: -0.37818, mean energy: -201.93002, varE: 21.05755\n",
      "step: 320, loss: -0.38257, mean energy: -201.04835, varE: 15.17842\n",
      "step: 330, loss: -0.73225, mean energy: -201.99564, varE: 8.70275\n",
      "step: 340, loss: 0.10490, mean energy: -201.33467, varE: 15.57591\n",
      "step: 350, loss: -0.46617, mean energy: -201.53869, varE: 11.00041\n",
      "Training took 47.244 hours\n"
     ]
    }
   ],
   "source": [
    "#DMRG results:  -202.5077381\n",
    "t0 = time.time()\n",
    "RNNEnergy, varRNNEnergy = run_2DTFIM_hyp(numsteps = 351, Nx = nx, Ny = ny,\n",
    "                                     Bx = 3, cell = 'HypGRU', num_units = 50, numsamples = 50, \n",
    "                                    lr1=1e-2, lr2 =1e-2, var_tol =30.0, seed = 111, fname = 'results_nov')\n",
    "t1 = time.time()\n",
    "print(f'Training took {np.round((t1-t0)/3600,3)} hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0998e0c0-ed3e-4b9f-94fa-1d76d97260f7",
   "metadata": {},
   "source": [
    "# 2D Euclidean RNN wavefunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c8d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 22:55:25.005066: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 79.56457, mean energy: -144.98793, varE: 1183.47906\n",
      "step: 10, loss: 16.38144, mean energy: -181.19605, varE: 117.46303\n",
      "step: 20, loss: 0.15567, mean energy: -197.30302, varE: 61.14489\n",
      "step: 30, loss: -2.11132, mean energy: -198.43163, varE: 43.51088\n",
      "step: 40, loss: -0.27820, mean energy: -199.24605, varE: 11.91682\n",
      "step: 50, loss: 0.05511, mean energy: -200.16404, varE: 11.72042\n",
      "step: 60, loss: 4.96292, mean energy: -200.36985, varE: 9.82231\n",
      "step: 70, loss: -0.30066, mean energy: -201.94728, varE: 6.73486\n",
      "step: 80, loss: -0.76502, mean energy: -201.50070, varE: 4.34933\n",
      "step: 90, loss: 4.21418, mean energy: -201.43603, varE: 5.98642\n",
      "step: 100, loss: -0.44566, mean energy: -201.97599, varE: 4.18374\n",
      "step: 110, loss: -0.54111, mean energy: -201.99824, varE: 3.28869\n",
      "step: 120, loss: 0.80403, mean energy: -202.54344, varE: 3.29128\n",
      "step: 130, loss: -0.06905, mean energy: -202.54185, varE: 3.04470\n",
      "step: 140, loss: 0.63722, mean energy: -202.82394, varE: 1.86760\n",
      "step: 150, loss: -1.19921, mean energy: -202.31083, varE: 1.78015\n",
      "step: 160, loss: 0.41134, mean energy: -202.17326, varE: 2.01132\n",
      "Best model saved at epoch 163 with best E=-202.35667, varE=1.16322\n",
      "step: 170, loss: 1.06443, mean energy: -202.20116, varE: 1.71402\n",
      "step: 180, loss: 0.63193, mean energy: -202.23664, varE: 1.98708\n",
      "step: 190, loss: 0.64105, mean energy: -202.38785, varE: 1.37729\n",
      "Best model saved at epoch 191 with best E=-202.38755, varE=1.09749\n",
      "step: 200, loss: 0.18121, mean energy: -202.18920, varE: 2.23647\n",
      "step: 210, loss: 1.71017, mean energy: -202.22314, varE: 2.77991\n",
      "Best model saved at epoch 219 with best E=-202.38946, varE=1.08336\n",
      "Best model saved at epoch 220 with best E=-202.66738, varE=1.14694\n",
      "step: 220, loss: -0.23567, mean energy: -202.66738, varE: 1.14694\n",
      "step: 230, loss: 0.60888, mean energy: -202.44467, varE: 1.37911\n",
      "step: 240, loss: -1.01082, mean energy: -202.69274, varE: 1.42512\n",
      "Best model saved at epoch 249 with best E=-202.81805, varE=1.04103\n",
      "step: 250, loss: -0.46936, mean energy: -202.38219, varE: 1.29771\n",
      "step: 260, loss: 0.73580, mean energy: -202.42566, varE: 1.87365\n",
      "step: 270, loss: 0.63662, mean energy: -202.21846, varE: 1.33232\n",
      "step: 280, loss: -0.05101, mean energy: -202.40992, varE: 1.34634\n",
      "step: 290, loss: -0.26579, mean energy: -202.50803, varE: 0.82350\n",
      "step: 300, loss: -1.14053, mean energy: -202.68353, varE: 1.04615\n",
      "step: 310, loss: 0.39557, mean energy: -202.55594, varE: 1.10808\n",
      "step: 320, loss: -0.65261, mean energy: -202.47369, varE: 0.86182\n",
      "step: 330, loss: 0.01307, mean energy: -202.41861, varE: 1.55690\n",
      "step: 340, loss: 0.31609, mean energy: -202.38866, varE: 0.80987\n",
      "step: 350, loss: -0.53621, mean energy: -202.49597, varE: 1.11143\n",
      "Training took 1.926 hours\n"
     ]
    }
   ],
   "source": [
    "from tfim2d_2drnn_train_loop import run_2DTFIM\n",
    "#DMRG results:  -202.5077381\n",
    "t0 = time.time()\n",
    "RNNEnergy, varRNNEnergy = run_2DTFIM(numsteps = 351, Nx = nx, Ny = ny,\n",
    "                                     Bx = 3, num_units = 50, numsamples = 50, \n",
    "                                     learningrate = 1e-2, var_tol = 1.2, seed = 111)\n",
    "t1 = time.time()\n",
    "print(f'Training took {np.round((t1-t0)/3600,3)} hours')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
