{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594eea4f-6853-4c7f-a5f8-5be4af2cd831",
   "metadata": {},
   "source": [
    "# 1D J1=1.0, J2=0.2, J3=0.5: 2nd set\n",
    "\n",
    "This notebook is part of the work arXiv:2505.22083 (https://arxiv.org/abs/2505.22083), \"Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz\". Code written by HLD. \n",
    "\n",
    "In this notebook, the original training were increased. No gradient clipping was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9abff75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../utility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fd866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_exact = -14.640825798\n",
    "syssize = 30 #30 is divisible by both 2 and 3\n",
    "nssamples = 50\n",
    "J1 = 1.0\n",
    "J2 = 0.2\n",
    "J3 = 0.5\n",
    "var_tol = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2f89e",
   "metadata": {},
   "source": [
    "# EuclGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6ae738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model every epoch\n",
    "from j1j2j3_hyprnn_train_loop_save_last import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1bb11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -1.66049, mean energy: 11.17103+0.45299j, varE: 1.69466\n",
      "step: 10, loss: -1.99533, mean energy: -1.15793+0.15225j, varE: 10.50147\n",
      "step: 20, loss: -0.45963, mean energy: -3.81849+0.18042j, varE: 7.21377\n",
      "step: 30, loss: -2.63721, mean energy: -6.24473+0.21212j, varE: 9.12473\n",
      "step: 40, loss: -4.70173, mean energy: -9.74107-0.17182j, varE: 4.82042\n",
      "step: 50, loss: 0.20715, mean energy: -10.15023-0.16776j, varE: 0.72845\n",
      "step: 60, loss: -0.02691, mean energy: -10.12712-0.01215j, varE: 0.59969\n",
      "step: 70, loss: 0.06728, mean energy: -10.09655-0.03140j, varE: 0.20407\n",
      "step: 80, loss: -0.53753, mean energy: -10.06489+0.00839j, varE: 0.58700\n",
      "step: 90, loss: 1.02023, mean energy: -9.78108+0.08925j, varE: 1.12993\n",
      "step: 100, loss: -0.37145, mean energy: -10.41280+0.21049j, varE: 4.22338\n",
      "step: 110, loss: -7.15878, mean energy: -11.77569+0.35958j, varE: 6.38037\n",
      "step: 120, loss: -0.18919, mean energy: -11.79294-0.23269j, varE: 6.81645\n",
      "step: 130, loss: -2.77904, mean energy: -11.86383+0.31089j, varE: 3.65308\n",
      "step: 140, loss: 2.90891, mean energy: -12.93127+0.18143j, varE: 4.97366\n",
      "step: 150, loss: 0.63180, mean energy: -12.74354+0.03974j, varE: 3.34646\n",
      "step: 160, loss: 2.61153, mean energy: -12.45972+0.11284j, varE: 2.46482\n",
      "step: 170, loss: 6.18489, mean energy: -12.98449-0.23251j, varE: 4.68093\n",
      "step: 180, loss: 5.92648, mean energy: -12.07993-0.05875j, varE: 2.33313\n",
      "step: 190, loss: -2.57594, mean energy: -13.70589+0.02017j, varE: 3.95834\n",
      "step: 200, loss: -1.60062, mean energy: -13.12393+0.10115j, varE: 1.83923\n",
      "step: 210, loss: -1.74443, mean energy: -13.79819-0.07814j, varE: 1.69038\n",
      "step: 220, loss: -7.53850, mean energy: -13.55865-0.01606j, varE: 2.07994\n",
      "step: 230, loss: -2.55141, mean energy: -14.42208+0.03798j, varE: 1.04862\n",
      "step: 240, loss: 1.90286, mean energy: -14.44911+0.06205j, varE: 1.08824\n",
      "step: 250, loss: -0.42892, mean energy: -14.60358-0.07163j, varE: 0.76965\n",
      "step: 260, loss: -2.98158, mean energy: -14.17032+0.05982j, varE: 1.36917\n",
      "step: 270, loss: 4.45232, mean energy: -13.96644+0.03683j, varE: 1.55884\n",
      "step: 280, loss: 0.37183, mean energy: -14.53197+0.02501j, varE: 1.48355\n",
      "step: 290, loss: -3.11624, mean energy: -14.40817-0.08308j, varE: 1.21343\n",
      "step: 300, loss: 0.04524, mean energy: -14.37228+0.02413j, varE: 1.06302\n",
      "step: 310, loss: -0.92511, mean energy: -14.48656-0.09176j, varE: 1.05150\n",
      "step: 320, loss: 4.92358, mean energy: -14.61468+0.00490j, varE: 1.50517\n",
      "step: 330, loss: -3.94788, mean energy: -14.42940-0.01888j, varE: 0.94445\n",
      "step: 340, loss: -0.65205, mean energy: -14.51193+0.04081j, varE: 0.77940\n",
      "step: 350, loss: 1.57059, mean energy: -14.60645-0.03637j, varE: 0.82682\n",
      "step: 360, loss: 0.48976, mean energy: -14.63840+0.03540j, varE: 1.08585\n",
      "step: 370, loss: 0.08171, mean energy: -14.64535+0.01278j, varE: 2.16173\n",
      "step: 380, loss: -2.22032, mean energy: -14.91636+0.01376j, varE: 1.88378\n",
      "step: 390, loss: 0.52066, mean energy: -14.68890+0.00350j, varE: 0.72234\n",
      "step: 400, loss: -0.44600, mean energy: -14.53517-0.00625j, varE: 0.91207\n",
      "step: 410, loss: 1.72449, mean energy: -14.36713+0.04870j, varE: 0.55278\n",
      "step: 420, loss: -0.98611, mean energy: -14.74123+0.04277j, varE: 0.73435\n",
      "step: 430, loss: 3.41605, mean energy: -14.57817-0.01797j, varE: 0.76655\n",
      "step: 440, loss: 0.46777, mean energy: -14.68093-0.04144j, varE: 0.74095\n",
      "step: 450, loss: 2.03773, mean energy: -14.80559+0.00116j, varE: 1.21747\n",
      "step: 460, loss: -1.52660, mean energy: -14.65387-0.00187j, varE: 0.64462\n",
      "step: 470, loss: -1.84572, mean energy: -14.52289+0.05703j, varE: 0.65247\n",
      "step: 480, loss: -1.18051, mean energy: -14.75451-0.01705j, varE: 0.67227\n",
      "step: 490, loss: 3.57541, mean energy: -14.43996-0.04507j, varE: 1.10835\n",
      "step: 500, loss: -3.69978, mean energy: -14.63140+0.14450j, varE: 0.78001\n",
      "step: 510, loss: 5.76028, mean energy: -14.57359+0.14178j, varE: 0.87362\n",
      "step: 520, loss: -1.87253, mean energy: -14.72531-0.03996j, varE: 0.41870\n",
      "step: 530, loss: 1.56085, mean energy: -14.80517-0.00329j, varE: 0.49075\n",
      "step: 540, loss: 0.59901, mean energy: -14.67309-0.01392j, varE: 0.52019\n",
      "step: 550, loss: -0.54028, mean energy: -14.80190+0.03049j, varE: 0.58979\n",
      "Total time taken: 3.398\n"
     ]
    }
   ],
   "source": [
    "cell_type = 'EuclGRU'\n",
    "hidden_units = 60\n",
    "wf_egru = rnn_eucl_wf(syssize, cell_type, hidden_units)\n",
    "\n",
    "nsteps = 551\n",
    "start = time.time()\n",
    "\n",
    "mE, vE = run_J1J2J3(wf=wf_egru, numsteps=nsteps, systemsize=syssize, var_tol=2.0, J1_  = J1, \n",
    "                   J2_ = J2, J3_ = J3, Marshall_sign = True, \n",
    "                  numsamples = nssamples, learningrate = 1e-2, seed = 111, fname = 'results_nov')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12817ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from j1j2j3_hyprnn_train_loop import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b76cde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -1.66049, mean energy: 11.17103+0.45299j, varE: 1.69466\n",
      "step: 10, loss: -1.99533, mean energy: -1.15793+0.15225j, varE: 10.50147\n",
      "step: 20, loss: -0.45963, mean energy: -3.81849+0.18042j, varE: 7.21377\n",
      "step: 30, loss: -2.63721, mean energy: -6.24473+0.21212j, varE: 9.12473\n",
      "step: 40, loss: -4.70173, mean energy: -9.74107-0.17182j, varE: 4.82042\n",
      "Best model saved at epoch 43 with best E=-9.75733+0.19444j, varE=1.69742\n",
      "Best model saved at epoch 46 with best E=-10.17657-0.26217j, varE=0.46003\n",
      "step: 50, loss: 0.20715, mean energy: -10.15023-0.16776j, varE: 0.72845\n",
      "Best model saved at epoch 52 with best E=-10.18415-0.10455j, varE=0.38929\n",
      "step: 60, loss: -0.02691, mean energy: -10.12712-0.01215j, varE: 0.59969\n",
      "step: 70, loss: 0.06728, mean energy: -10.09655-0.03140j, varE: 0.20407\n",
      "step: 80, loss: -0.53753, mean energy: -10.06489+0.00839j, varE: 0.58700\n",
      "step: 90, loss: 1.02023, mean energy: -9.78108+0.08925j, varE: 1.12993\n",
      "Best model saved at epoch 95 with best E=-10.28242-0.18546j, varE=0.91368\n",
      "step: 100, loss: -0.37145, mean energy: -10.41280+0.21049j, varE: 4.22338\n",
      "step: 110, loss: -7.15878, mean energy: -11.77569+0.35958j, varE: 6.38037\n",
      "step: 120, loss: -0.18919, mean energy: -11.79294-0.23269j, varE: 6.81645\n",
      "step: 130, loss: -2.77904, mean energy: -11.86383+0.31089j, varE: 3.65308\n",
      "Best model saved at epoch 134 with best E=-12.28945-0.01912j, varE=1.49396\n",
      "step: 140, loss: 2.90891, mean energy: -12.93127+0.18143j, varE: 4.97366\n",
      "step: 150, loss: 0.63180, mean energy: -12.74354+0.03974j, varE: 3.34646\n",
      "Best model saved at epoch 151 with best E=-12.64898+0.15631j, varE=1.96756\n",
      "Best model saved at epoch 157 with best E=-12.79135-0.02458j, varE=1.44167\n",
      "step: 160, loss: 2.61153, mean energy: -12.45972+0.11284j, varE: 2.46482\n",
      "step: 170, loss: 6.18489, mean energy: -12.98449-0.23251j, varE: 4.68093\n",
      "step: 180, loss: 5.92648, mean energy: -12.07993-0.05875j, varE: 2.33313\n",
      "Best model saved at epoch 187 with best E=-13.14414+0.01852j, varE=1.44922\n",
      "step: 190, loss: -2.57594, mean energy: -13.70589+0.02017j, varE: 3.95834\n",
      "Best model saved at epoch 196 with best E=-13.32905-0.13633j, varE=1.68962\n",
      "Best model saved at epoch 198 with best E=-13.40666+0.06821j, varE=1.96218\n",
      "step: 200, loss: -1.60062, mean energy: -13.12393+0.10115j, varE: 1.83923\n",
      "Best model saved at epoch 206 with best E=-13.96702+0.04359j, varE=1.66322\n",
      "step: 210, loss: -1.74443, mean energy: -13.79819-0.07814j, varE: 1.69038\n",
      "Best model saved at epoch 217 with best E=-14.13115+0.03574j, varE=1.84473\n",
      "step: 220, loss: -7.53850, mean energy: -13.55865-0.01606j, varE: 2.07994\n",
      "Best model saved at epoch 226 with best E=-14.14592+0.00071j, varE=1.58268\n",
      "Best model saved at epoch 228 with best E=-14.42201-0.04570j, varE=1.94796\n",
      "Best model saved at epoch 230 with best E=-14.42208+0.03798j, varE=1.04862\n",
      "step: 230, loss: -2.55141, mean energy: -14.42208+0.03798j, varE: 1.04862\n",
      "Best model saved at epoch 240 with best E=-14.44911+0.06205j, varE=1.08824\n",
      "step: 240, loss: 1.90286, mean energy: -14.44911+0.06205j, varE: 1.08824\n",
      "Best model saved at epoch 244 with best E=-14.55171-0.04190j, varE=1.08214\n",
      "Best model saved at epoch 250 with best E=-14.60358-0.07163j, varE=0.76965\n",
      "step: 250, loss: -0.42892, mean energy: -14.60358-0.07163j, varE: 0.76965\n",
      "step: 260, loss: -2.98158, mean energy: -14.17032+0.05982j, varE: 1.36917\n",
      "step: 270, loss: 4.45232, mean energy: -13.96644+0.03683j, varE: 1.55884\n",
      "step: 280, loss: 0.37183, mean energy: -14.53197+0.02501j, varE: 1.48355\n",
      "Best model saved at epoch 282 with best E=-14.61090+0.07278j, varE=1.76012\n",
      "Best model saved at epoch 285 with best E=-14.71057+0.09513j, varE=0.69578\n",
      "step: 290, loss: -3.11624, mean energy: -14.40817-0.08308j, varE: 1.21343\n",
      "step: 300, loss: 0.04524, mean energy: -14.37228+0.02413j, varE: 1.06302\n",
      "step: 310, loss: -0.92511, mean energy: -14.48656-0.09176j, varE: 1.05150\n",
      "Best model saved at epoch 315 with best E=-14.81569-0.14650j, varE=1.91389\n",
      "step: 320, loss: 4.92358, mean energy: -14.61468+0.00490j, varE: 1.50517\n",
      "step: 330, loss: -3.94788, mean energy: -14.42940-0.01888j, varE: 0.94445\n",
      "step: 340, loss: -0.65205, mean energy: -14.51193+0.04081j, varE: 0.77940\n",
      "Best model saved at epoch 345 with best E=-14.88793-0.11312j, varE=1.13538\n",
      "step: 350, loss: 1.57059, mean energy: -14.60645-0.03637j, varE: 0.82682\n",
      "step: 360, loss: 0.48976, mean energy: -14.63840+0.03540j, varE: 1.08585\n",
      "step: 370, loss: 0.08171, mean energy: -14.64535+0.01278j, varE: 2.16173\n",
      "Best model saved at epoch 376 with best E=-14.97112+0.04684j, varE=1.10607\n",
      "step: 380, loss: -2.22032, mean energy: -14.91636+0.01376j, varE: 1.88378\n",
      "step: 390, loss: 0.52066, mean energy: -14.68890+0.00350j, varE: 0.72234\n",
      "step: 400, loss: -0.44600, mean energy: -14.53517-0.00625j, varE: 0.91207\n",
      "step: 410, loss: 1.72449, mean energy: -14.36713+0.04870j, varE: 0.55278\n",
      "step: 420, loss: -0.98611, mean energy: -14.74123+0.04277j, varE: 0.73435\n",
      "step: 430, loss: 3.41605, mean energy: -14.57817-0.01797j, varE: 0.76655\n",
      "step: 440, loss: 0.46777, mean energy: -14.68093-0.04144j, varE: 0.74095\n",
      "step: 450, loss: 2.03773, mean energy: -14.80559+0.00116j, varE: 1.21747\n",
      "step: 460, loss: -1.52660, mean energy: -14.65387-0.00187j, varE: 0.64462\n",
      "step: 470, loss: -1.84572, mean energy: -14.52289+0.05703j, varE: 0.65247\n",
      "step: 480, loss: -1.18051, mean energy: -14.75451-0.01705j, varE: 0.67227\n",
      "step: 490, loss: 3.57541, mean energy: -14.43996-0.04507j, varE: 1.10835\n",
      "step: 500, loss: -3.69978, mean energy: -14.63140+0.14450j, varE: 0.78001\n",
      "step: 510, loss: 5.76028, mean energy: -14.57359+0.14178j, varE: 0.87362\n",
      "step: 520, loss: -1.87253, mean energy: -14.72531-0.03996j, varE: 0.41870\n",
      "step: 530, loss: 1.56085, mean energy: -14.80517-0.00329j, varE: 0.49075\n",
      "step: 540, loss: 0.59901, mean energy: -14.67309-0.01392j, varE: 0.52019\n",
      "step: 550, loss: -0.54028, mean energy: -14.80190+0.03049j, varE: 0.58979\n",
      "Total time taken: 4.714\n"
     ]
    }
   ],
   "source": [
    "#var_tol = 2.0, up to 550 epochs (but best model saved at e376)\n",
    "cell_type = 'EuclGRU'\n",
    "hidden_units = 60\n",
    "wf_egru = rnn_eucl_wf(syssize, cell_type, hidden_units)\n",
    "\n",
    "nsteps = 551\n",
    "start = time.time()\n",
    "\n",
    "mE, vE = run_J1J2J3(wf=wf_egru, numsteps=nsteps, systemsize=syssize, var_tol=2.0, J1_  = J1, \n",
    "                   J2_ = J2, J3_ = J3, Marshall_sign = True, \n",
    "                  numsamples = nssamples, learningrate = 1e-2, seed = 111, fname = 'results_nov')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587abf4",
   "metadata": {},
   "source": [
    "# HypGRU 55 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5acd0a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:39:48.227307: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hldao/opt/anaconda3/lib/python3.9/site-packages/google/api_core/_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.12) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# saving the model every epoch\n",
    "from j1j2j3_hyprnn_train_loop_save_last import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e157a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -3.70720, mean energy: 10.34374-0.60345j, varE: 4.60298\n",
      "step: 10, loss: 1.08876, mean energy: -1.98504+0.07080j, varE: 9.33381\n",
      "step: 20, loss: 4.13452, mean energy: -1.86714+0.42712j, varE: 10.54214\n",
      "step: 30, loss: -5.93134, mean energy: -4.61488+0.02444j, varE: 12.55273\n",
      "step: 40, loss: 0.78404, mean energy: -9.48739+0.09153j, varE: 9.06663\n",
      "step: 50, loss: 5.36865, mean energy: -11.21624-0.03758j, varE: 6.57377\n",
      "step: 60, loss: 0.00812, mean energy: -11.47818+0.03909j, varE: 6.91036\n",
      "step: 70, loss: -1.80083, mean energy: -12.79751+0.00483j, varE: 5.08553\n",
      "step: 80, loss: -5.08958, mean energy: -13.03098-0.01217j, varE: 2.42115\n",
      "step: 90, loss: -2.89160, mean energy: -13.04657+0.19909j, varE: 2.37322\n",
      "step: 100, loss: 1.24956, mean energy: -13.26341+0.00827j, varE: 4.19102\n",
      "step: 110, loss: 0.14029, mean energy: -13.42810+0.05461j, varE: 2.71722\n",
      "step: 120, loss: 4.91045, mean energy: -13.24100-0.04200j, varE: 2.47961\n",
      "step: 130, loss: 0.22856, mean energy: -13.72739-0.02414j, varE: 2.81917\n",
      "step: 140, loss: 0.98834, mean energy: -13.43974-0.04659j, varE: 2.90100\n",
      "step: 150, loss: -0.09389, mean energy: -13.96422-0.01813j, varE: 0.96459\n",
      "step: 160, loss: 0.41238, mean energy: -13.83751-0.03193j, varE: 1.15651\n",
      "step: 170, loss: -3.29260, mean energy: -13.79821+0.03977j, varE: 1.08041\n",
      "step: 180, loss: -0.34351, mean energy: -13.41424+0.02925j, varE: 1.63836\n",
      "step: 190, loss: 4.51067, mean energy: -14.00072+0.11187j, varE: 1.01591\n",
      "step: 200, loss: 0.46259, mean energy: -14.06848+0.08927j, varE: 0.47864\n",
      "step: 210, loss: -2.36200, mean energy: -14.02209-0.07934j, varE: 1.11936\n",
      "step: 220, loss: -1.50269, mean energy: -13.68493-0.13816j, varE: 1.28932\n",
      "step: 230, loss: -0.91911, mean energy: -14.11459+0.14099j, varE: 1.47536\n",
      "step: 240, loss: -0.65791, mean energy: -13.78876+0.19382j, varE: 1.11080\n",
      "step: 250, loss: -0.39141, mean energy: -14.13027+0.09437j, varE: 0.36737\n",
      "step: 260, loss: -2.44556, mean energy: -14.29161-0.03309j, varE: 0.70572\n",
      "step: 270, loss: 2.51611, mean energy: -14.25676+0.13005j, varE: 0.95565\n",
      "step: 280, loss: -2.61409, mean energy: -14.27671+0.06995j, varE: 0.37493\n",
      "step: 290, loss: -3.05850, mean energy: -14.46729-0.05200j, varE: 1.60060\n",
      "step: 300, loss: -4.51576, mean energy: -14.02074-0.07115j, varE: 1.00044\n",
      "step: 310, loss: 2.30446, mean energy: -14.30477+0.03522j, varE: 0.89433\n",
      "step: 320, loss: 6.66423, mean energy: -14.33717+0.11342j, varE: 0.81885\n",
      "step: 330, loss: 0.94852, mean energy: -14.42920+0.00015j, varE: 0.49360\n",
      "step: 340, loss: -4.08466, mean energy: -14.07301-0.10792j, varE: 0.83422\n",
      "step: 350, loss: -1.73964, mean energy: -14.16207+0.08031j, varE: 1.11398\n",
      "step: 360, loss: -1.96597, mean energy: -14.49753-0.13471j, varE: 1.61651\n",
      "step: 370, loss: -0.07501, mean energy: -14.41734+0.02620j, varE: 0.43981\n",
      "step: 380, loss: -0.85144, mean energy: -14.51225-0.09415j, varE: 0.68985\n",
      "step: 390, loss: -3.16222, mean energy: -14.29006-0.01472j, varE: 0.63306\n",
      "step: 400, loss: 2.63905, mean energy: -14.52871+0.02099j, varE: 0.66203\n",
      "step: 410, loss: 3.70743, mean energy: -14.54624-0.01603j, varE: 0.74115\n",
      "step: 420, loss: -5.08759, mean energy: -14.59847-0.08409j, varE: 1.30604\n",
      "step: 430, loss: -3.43532, mean energy: -14.46882+0.02295j, varE: 0.61244\n",
      "step: 440, loss: 5.48422, mean energy: -14.95539-0.00076j, varE: 5.40158\n",
      "step: 450, loss: 3.09090, mean energy: -14.71881+0.06316j, varE: 2.37203\n",
      "step: 460, loss: 1.83455, mean energy: -14.28237+0.12587j, varE: 1.07838\n",
      "step: 470, loss: 16.64827, mean energy: -14.30454+0.31922j, varE: 2.35866\n",
      "step: 480, loss: 3.45294, mean energy: -14.57033+0.09852j, varE: 0.83672\n",
      "step: 490, loss: 2.99725, mean energy: -14.38513-0.14050j, varE: 0.67976\n",
      "step: 500, loss: -1.14787, mean energy: -14.71213+0.03630j, varE: 0.71768\n",
      "step: 510, loss: -0.77512, mean energy: -14.51198-0.02998j, varE: 0.50489\n",
      "step: 520, loss: -2.70734, mean energy: -14.70251-0.05081j, varE: 0.73632\n",
      "step: 530, loss: 1.14110, mean energy: -14.67760+0.09628j, varE: 0.82429\n",
      "step: 540, loss: -2.27483, mean energy: -14.57120-0.05269j, varE: 0.63664\n",
      "step: 550, loss: -1.03966, mean energy: -14.77705-0.12269j, varE: 1.51825\n",
      "Total time taken: 17.552\n"
     ]
    }
   ],
   "source": [
    "cell_type = 'HypGRU'\n",
    "hidden_units = 55\n",
    "wf_hgru = rnn_hyp_wf(syssize, cell_type, 'hyp', 'id', hidden_units)\n",
    "nsteps=551\n",
    "start = time.time()\n",
    "mE, vE = run_J1J2J3_hypvars(wf=wf_hgru, numsteps=nsteps, systemsize=syssize, var_tol=2.0,\n",
    "                          J1_ = J1, J2_ = J2, J3_ = J3, Marshall_sign = True, \n",
    "                           numsamples = nssamples,  lr1=1e-2, lr2=1e-2, seed = 111, fname = 'results_nov')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb8fee10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: -3.70720, mean energy: 10.34374-0.60345j, varE: 4.60298\n",
      "step: 10, loss: 1.08876, mean energy: -1.98504+0.07080j, varE: 9.33381\n",
      "step: 20, loss: 4.13452, mean energy: -1.86714+0.42712j, varE: 10.54214\n",
      "step: 30, loss: -5.93134, mean energy: -4.61488+0.02444j, varE: 12.55273\n",
      "step: 40, loss: 0.78404, mean energy: -9.48739+0.09153j, varE: 9.06663\n",
      "step: 50, loss: 5.36865, mean energy: -11.21624-0.03758j, varE: 6.57377\n",
      "step: 60, loss: 0.00812, mean energy: -11.47818+0.03909j, varE: 6.91036\n",
      "step: 70, loss: -1.80083, mean energy: -12.79751+0.00483j, varE: 5.08553\n",
      "step: 80, loss: -5.08958, mean energy: -13.03098-0.01217j, varE: 2.42115\n",
      "Best model saved at epoch 84 with best E=-13.42585-0.16812j, varE=1.69840\n",
      "step: 90, loss: -2.89160, mean energy: -13.04657+0.19909j, varE: 2.37322\n",
      "step: 100, loss: 1.24956, mean energy: -13.26341+0.00827j, varE: 4.19102\n",
      "step: 110, loss: 0.14029, mean energy: -13.42810+0.05461j, varE: 2.71722\n",
      "Best model saved at epoch 113 with best E=-13.59780+0.26976j, varE=1.83796\n",
      "step: 120, loss: 4.91045, mean energy: -13.24100-0.04200j, varE: 2.47961\n",
      "step: 130, loss: 0.22856, mean energy: -13.72739-0.02414j, varE: 2.81917\n",
      "step: 140, loss: 0.98834, mean energy: -13.43974-0.04659j, varE: 2.90100\n",
      "Best model saved at epoch 141 with best E=-13.70979+0.04727j, varE=1.70437\n",
      "Best model saved at epoch 150 with best E=-13.96422-0.01813j, varE=0.96459\n",
      "step: 150, loss: -0.09389, mean energy: -13.96422-0.01813j, varE: 0.96459\n",
      "Best model saved at epoch 151 with best E=-14.03463+0.02412j, varE=1.60854\n",
      "step: 160, loss: 0.41238, mean energy: -13.83751-0.03193j, varE: 1.15651\n",
      "step: 170, loss: -3.29260, mean energy: -13.79821+0.03977j, varE: 1.08041\n",
      "step: 180, loss: -0.34351, mean energy: -13.41424+0.02925j, varE: 1.63836\n",
      "step: 190, loss: 4.51067, mean energy: -14.00072+0.11187j, varE: 1.01591\n",
      "Best model saved at epoch 192 with best E=-14.05264+0.11499j, varE=1.08633\n",
      "Best model saved at epoch 195 with best E=-14.22446+0.04634j, varE=1.47668\n",
      "step: 200, loss: 0.46259, mean energy: -14.06848+0.08927j, varE: 0.47864\n",
      "Best model saved at epoch 208 with best E=-14.28549+0.06440j, varE=0.91286\n",
      "step: 210, loss: -2.36200, mean energy: -14.02209-0.07934j, varE: 1.11936\n",
      "step: 220, loss: -1.50269, mean energy: -13.68493-0.13816j, varE: 1.28932\n",
      "step: 230, loss: -0.91911, mean energy: -14.11459+0.14099j, varE: 1.47536\n",
      "step: 240, loss: -0.65791, mean energy: -13.78876+0.19382j, varE: 1.11080\n",
      "step: 250, loss: -0.39141, mean energy: -14.13027+0.09437j, varE: 0.36737\n",
      "Best model saved at epoch 255 with best E=-14.44092-0.12544j, varE=0.82334\n",
      "step: 260, loss: -2.44556, mean energy: -14.29161-0.03309j, varE: 0.70572\n",
      "step: 270, loss: 2.51611, mean energy: -14.25676+0.13005j, varE: 0.95565\n",
      "step: 280, loss: -2.61409, mean energy: -14.27671+0.06995j, varE: 0.37493\n",
      "Best model saved at epoch 290 with best E=-14.46729-0.05200j, varE=1.60060\n",
      "step: 290, loss: -3.05850, mean energy: -14.46729-0.05200j, varE: 1.60060\n",
      "step: 300, loss: -4.51576, mean energy: -14.02074-0.07115j, varE: 1.00044\n",
      "step: 310, loss: 2.30446, mean energy: -14.30477+0.03522j, varE: 0.89433\n",
      "step: 320, loss: 6.66423, mean energy: -14.33717+0.11342j, varE: 0.81885\n",
      "step: 330, loss: 0.94852, mean energy: -14.42920+0.00015j, varE: 0.49360\n",
      "Best model saved at epoch 333 with best E=-14.60127-0.14246j, varE=1.91621\n",
      "step: 340, loss: -4.08466, mean energy: -14.07301-0.10792j, varE: 0.83422\n",
      "Best model saved at epoch 348 with best E=-14.66453+0.03808j, varE=1.34367\n",
      "step: 350, loss: -1.73964, mean energy: -14.16207+0.08031j, varE: 1.11398\n",
      "step: 360, loss: -1.96597, mean energy: -14.49753-0.13471j, varE: 1.61651\n",
      "step: 370, loss: -0.07501, mean energy: -14.41734+0.02620j, varE: 0.43981\n",
      "step: 380, loss: -0.85144, mean energy: -14.51225-0.09415j, varE: 0.68985\n",
      "step: 390, loss: -3.16222, mean energy: -14.29006-0.01472j, varE: 0.63306\n",
      "step: 400, loss: 2.63905, mean energy: -14.52871+0.02099j, varE: 0.66203\n",
      "step: 410, loss: 3.70743, mean energy: -14.54624-0.01603j, varE: 0.74115\n",
      "Best model saved at epoch 411 with best E=-14.71581-0.01080j, varE=1.87789\n",
      "step: 420, loss: -5.08759, mean energy: -14.59847-0.08409j, varE: 1.30604\n",
      "step: 430, loss: -3.43532, mean energy: -14.46882+0.02295j, varE: 0.61244\n",
      "step: 440, loss: 5.48422, mean energy: -14.95539-0.00076j, varE: 5.40158\n",
      "step: 450, loss: 3.09090, mean energy: -14.71881+0.06316j, varE: 2.37203\n",
      "step: 460, loss: 1.83455, mean energy: -14.28237+0.12587j, varE: 1.07838\n",
      "step: 470, loss: 16.64827, mean energy: -14.30454+0.31922j, varE: 2.35866\n",
      "Best model saved at epoch 471 with best E=-14.76643-0.04362j, varE=1.32404\n",
      "step: 480, loss: 3.45294, mean energy: -14.57033+0.09852j, varE: 0.83672\n",
      "step: 490, loss: 2.99725, mean energy: -14.38513-0.14050j, varE: 0.67976\n",
      "step: 500, loss: -1.14787, mean energy: -14.71213+0.03630j, varE: 0.71768\n",
      "step: 510, loss: -0.77512, mean energy: -14.51198-0.02998j, varE: 0.50489\n",
      "Best model saved at epoch 519 with best E=-14.84846-0.01421j, varE=0.73719\n",
      "step: 520, loss: -2.70734, mean energy: -14.70251-0.05081j, varE: 0.73632\n",
      "step: 530, loss: 1.14110, mean energy: -14.67760+0.09628j, varE: 0.82429\n",
      "Best model saved at epoch 537 with best E=-14.89932+0.08902j, varE=1.07022\n",
      "step: 540, loss: -2.27483, mean energy: -14.57120-0.05269j, varE: 0.63664\n",
      "step: 550, loss: -1.03966, mean energy: -14.77705-0.12269j, varE: 1.51825\n",
      "Total time taken: 27.64\n"
     ]
    }
   ],
   "source": [
    "# Increase the training duration to 551, saved 2 models: one at epoch 411 and one at the automatically-saving-best epoch\n",
    "cell_type = 'HypGRU'\n",
    "hidden_units = 55\n",
    "wf_hgru = rnn_hyp_wf(syssize, cell_type, 'hyp', 'id', hidden_units)\n",
    "nsteps=551\n",
    "start = time.time()\n",
    "mE, vE = run_J1J2J3_hypvars(wf=wf_hgru, numsteps=nsteps, systemsize=syssize, var_tol=2.0,\n",
    "                          J1_ = J1, J2_ = J2, J3_ = J3, Marshall_sign = True, \n",
    "                           numsamples = nssamples,  lr1=1e-2, lr2=1e-2, seed = 111, fname = 'results_nov')\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print(f'Total time taken: {np.round(duration/3600,3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
